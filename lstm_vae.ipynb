{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocessing.wrangling import get_indi_df, get_labels, slide_and_flatten\n",
    "from preprocessing.extract_features import get_all_ta_features, get_wavelet_coeffs\n",
    "from evaluation.eval import sliding_window_cv_regression, batch_test_swcv_regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import datetime\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from math import prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_closing_price(y, cls_price):\n",
    "    return y + cls_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersistanceModel:\n",
    "    def __init__(self, persist_colname='Close'):\n",
    "        self.persist_colname = persist_colname\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"PersistanceModel(persist_colname={})\".format(self.persist_colname)\n",
    "\n",
    "    def fit(self, Xtr, ytr):\n",
    "        pass\n",
    "\n",
    "    def predict(self, Xts):\n",
    "        return Xts.loc[:, self.persist_colname]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims, op_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(latent_dims, op_dim//2)\n",
    "        self.linear2 = nn.Linear(op_dim//2, op_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.relu(self.linear1(z))\n",
    "        z = torch.sigmoid(self.linear2(z))\n",
    "        # return z.reshape((-1, 1, 28, 28))\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, ip_dim, latent_dims):\n",
    "        assert ip_dim//2 >= latent_dims, \"Ensure ip_dim//2 () >= latent_dims ()\".format(ip_dim//2, latent_dims)\n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(ip_dim, ip_dim//2)\n",
    "        self.linear2 = nn.Linear(ip_dim//2, latent_dims)\n",
    "        self.linear3 = nn.Linear(ip_dim//2, latent_dims)\n",
    "\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        # self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
    "        # self.N.scale = self.N.scale.cuda()\n",
    "        self.kl = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        print('xmax', torch.max(x), 'xmin', torch.min(x))\n",
    "        print('pfltn', x)\n",
    "        x = self.linear1(x)\n",
    "        print('pl1', x)\n",
    "        x = F.relu(x)\n",
    "        print('prelu', x)\n",
    "        mu =  self.linear2(x)\n",
    "        sigma = torch.exp(self.linear3(x))\n",
    "        z = mu + sigma*self.N.sample(mu.shape)\n",
    "        print('x', x, 'mu', mu, 'sigma', sigma, 'z', z)\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, ip_dim, latent_dims):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = VariationalEncoder(ip_dim, latent_dims)\n",
    "        self.decoder = Decoder(latent_dims, ip_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(autoencoder, data, epochs=20):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    opt = torch.optim.Adam(autoencoder.parameters())\n",
    "    for epoch in range(epochs):\n",
    "        x = data  # Full batch gradient descent\n",
    "    # for x in data:\n",
    "        x = x.to(device) # GPU\n",
    "        opt.zero_grad()\n",
    "        print(x)\n",
    "        x_hat = autoencoder(x)\n",
    "        print(x, x_hat)\n",
    "        loss = ((x.reshape(x_hat.shape) - x_hat)**2).sum() + autoencoder.encoder.kl\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_cv_torch(swdf, y, model, optimizer, loss_fn, n_tr, n_ts=1, scorers=[], comment=\"\", post_processor=None):\n",
    "    assert len(swdf) == len(y), \"Length of X ([]) must match that of y ([]).\".format(len(X), len(y))\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    y_pred = []\n",
    "    y_target = []\n",
    "    agg_results = {}\n",
    "    if post_processor is not None:\n",
    "        post_processor_f, post_processor_args = post_processor[0], post_processor[1]\n",
    "        \n",
    "\n",
    "    for i_tr_start in range(0, len(swdf)-(n_tr+n_ts)):\n",
    "        # The last i_ts_end should be len(X).\n",
    "        # i_ts_end = i_ts_start + n_ts\n",
    "        # Now, i_tr_end = i_ts_start\n",
    "        # So, i_tr_start = i_ts_start - n_tr\n",
    "        # But, i_ts_start = i_ts_end - n_ts\n",
    "        # Thus, i_tr_start = i_ts_end - n_tr - n_ts\n",
    "        # Hence, last i_tr_start = len(X) - (n_tr + n_ts)\n",
    "\n",
    "        i_tr_end = i_ts_start = i_tr_start + n_tr \n",
    "        i_ts_end = i_ts_start + n_ts \n",
    "\n",
    "        Xtr, Xts = swdf[i_tr_start:i_tr_end, :, :], swdf[i_ts_start:i_ts_end, :, :]\n",
    "        ytr, yts = y[i_tr_start:i_tr_end].to_numpy(), y[i_ts_start:i_ts_end].to_numpy()\n",
    "        Xtr, Xts, ytr, yts = torch.Tensor(Xtr), torch.Tensor(Xts), torch.Tensor(ytr), torch.Tensor(yts)\n",
    "        Xtr, Xts, ytr, yts = Xtr.float().to(device), Xts.float().to(device), ytr.float().to(device), yts.float().to(device)\n",
    "        model.to(device)\n",
    "        \n",
    "        epochs = 150\n",
    "        for e in range(epochs):\n",
    "            model.train()\n",
    "            pred = model(Xtr)\n",
    "            loss = loss_fn(pred, ytr)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(Xts)\n",
    "            mape = torch.mean((torch.abs((yts - pred) / yts)) * 100)\n",
    "            if device == \"cuda\":\n",
    "                pred = pred.detach()\n",
    "                yts = yts.detach()\n",
    "\n",
    "            if len(pred.shape) == 0:\n",
    "                pred = pred.unsqueeze(0)\n",
    "            if pred.shape[0] == 1:\n",
    "                y_pred.append(pred.item())\n",
    "                y_target.append(yts.item())\n",
    "            else:\n",
    "                y_pred, yts = list(y_pred), list(yts)\n",
    "                y_pred.extend(y_pred)\n",
    "                y_target.extend(yts)\n",
    "\n",
    "    # print(len(y_target), len(y_pred))\n",
    "    # print(y_pred, y_target)\n",
    "    if len(y_pred) > 1:\n",
    "        y_pred = np.squeeze(y_pred)\n",
    "\n",
    "    if post_processor is not None:\n",
    "        y_pred = post_processor_f(y_pred, **post_processor_args)\n",
    "        y_target = post_processor_f(y_target, **post_processor_args)\n",
    "\n",
    "    print(\"y_target, y_pred\", y_target, y_pred)\n",
    "    agg_results['time'] = datetime.datetime.now()\n",
    "    agg_results['model'] = str(model)\n",
    "    agg_results['comment'] = comment\n",
    "    for scorer in scorers:\n",
    "        agg_results[scorer.__name__] = scorer(y_target, y_pred)\n",
    "\n",
    "    return agg_results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBaseline(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMBaseline, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.dense = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, swdf):\n",
    "        _, hncn = self.lstm(swdf)\n",
    "        hn, cn = hncn\n",
    "        hn = hn.squeeze()\n",
    "        op = self.dense(hn)\n",
    "        return op.squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5156, 240.9499969482422, 236.25, 249.3000030517578, 252.1999969482422, 240.0, 239.85000610351562, 233.89999389648438, 252.1999969482422, 251.8000030517578, 243.6999969482422, 229.60000610351562, 220.39999389648438, 222.10000610351562, 210.85000610351562, 202.9499969482422, 202.14999389648438, 219.14999389648438, 222.25, 218.14999389648438, 226.64999389648438, 240.64999389648438, 250.25, 233.4499969482422, 247.6999969482422, 263.25, 259.8500061035156, 268.75, 267.5, 283.5, 268.6499938964844, 268.0, 263.20001220703125, 271.6499938964844, 290.6000061035156, 311.25, 350.8500061035156, 331.3500061035156, 335.3999938964844, 337.95001220703125, 328.20001220703125, 319.29998779296875, 325.8500061035156, 346.25, 347.1499938964844, 356.95001220703125, 394.70001220703125, 379.29998779296875, 381.1000061035156, 373.95001220703125, 370.1000061035156, 352.3999938964844, 349.3999938964844, 342.5, 352.79998779296875, 350.5, 353.20001220703125, 343.8999938964844, 349.95001220703125, 341.3500061035156, 330.25, 340.54998779296875, 336.3500061035156, 338.1000061035156, 345.1000061035156, 308.79998779296875, 306.8999938964844, 310.95001220703125, 300.04998779296875, 307.5, 301.3999938964844, 300.6000061035156, 297.3500061035156, 290.8999938964844, 288.20001220703125, 287.29998779296875, 288.0, 289.6499938964844, 288.6000061035156, 292.1000061035156, 294.95001220703125, 293.3999938964844, 298.04998779296875, 311.1000061035156, 320.04998779296875, 305.79998779296875, 306.0, 317.5, 312.8999938964844, 314.45001220703125, 307.1000061035156, 305.45001220703125, 300.8500061035156, 307.04998779296875, 311.79998779296875, 301.3999938964844, 303.70001220703125, 305.1000061035156, 305.29998779296875, 298.75, 277.20001220703125, 267.45001220703125, 267.95001220703125, 254.8000030517578, 265.0, 280.45001220703125, 273.54998779296875, 273.0, 285.75, 292.3500061035156, 306.0, 314.54998779296875, 318.79998779296875, 325.45001220703125, 327.8500061035156, 320.6000061035156, 320.75, 303.75, 313.0, 323.70001220703125, 319.1499938964844, 316.04998779296875, 310.29998779296875, 307.75, 299.1499938964844, 296.20001220703125, 288.95001220703125, 293.25, 289.45001220703125, 300.8999938964844, 308.29998779296875, 312.6000061035156, 321.0, 330.8999938964844, 340.1000061035156, 344.95001220703125, 339.1499938964844, 337.75, 344.25, 343.54998779296875, 352.6499938964844, 361.45001220703125, 352.6000061035156, 349.8999938964844, 353.8999938964844, 358.75, 368.1499938964844, 366.54998779296875, 367.0, 377.20001220703125, 372.29998779296875, 376.6000061035156, 393.3999938964844, 411.0, 403.8999938964844, 399.95001220703125, 414.8999938964844, 423.20001220703125, 410.29998779296875, 412.1499938964844, 410.1499938964844, 406.3500061035156, 408.20001220703125, 374.29998779296875, 396.70001220703125, 399.04998779296875, 398.1499938964844, 406.1499938964844, 413.20001220703125, 406.1000061035156, 402.25, 400.20001220703125, 397.0, 401.8500061035156, 396.0, 405.79998779296875, 419.3999938964844, 408.3999938964844, 391.45001220703125, 375.1000061035156, 373.1499938964844, 368.54998779296875, 356.0, 362.29998779296875, 359.79998779296875, 340.70001220703125, 314.1000061035156, 309.54998779296875, 303.70001220703125, 311.1499938964844, 309.45001220703125, 323.25, 339.3500061035156, 342.8500061035156, 346.8500061035156, 329.54998779296875, 330.6499938964844, 328.29998779296875, 331.20001220703125, 332.6499938964844, 337.5, 339.95001220703125, 350.1499938964844, 348.25, 345.6499938964844, 337.25, 326.8999938964844, 325.29998779296875, 335.1499938964844, 342.20001220703125, 345.25, 338.3999938964844, 345.20001220703125, 354.8999938964844, 357.3999938964844, 346.3500061035156, 351.8999938964844, 350.8999938964844, 349.70001220703125, 349.20001220703125, 339.3999938964844, 339.5, 335.8999938964844, 337.70001220703125, 346.04998779296875, 352.6000061035156, 364.6499938964844, 357.45001220703125, 349.95001220703125, 345.6000061035156, 336.45001220703125, 338.8999938964844, 350.8999938964844, 335.45001220703125, 337.6000061035156, 349.75, 355.04998779296875, 358.95001220703125, 327.54998779296875, 331.8999938964844, 326.20001220703125, 322.6499938964844, 304.8999938964844, 309.1499938964844, 309.5, 313.45001220703125, 313.1499938964844, 318.8999938964844, 328.5, 332.1000061035156, 329.75, 318.8500061035156, 317.6499938964844, 311.25, 294.75, 297.3500061035156, 295.95001220703125, 290.95001220703125, 288.1499938964844, 280.54998779296875, 284.29998779296875, 291.20001220703125, 289.54998779296875, 294.1000061035156, 299.6499938964844, 298.70001220703125, 294.1000061035156, 292.1000061035156, 295.3500061035156, 303.5, 307.3999938964844, 302.8500061035156, 311.70001220703125, 317.70001220703125, 314.8500061035156, 320.0, 316.5, 317.5, 320.29998779296875, 319.3999938964844, 316.8500061035156, 321.95001220703125, 320.8999938964844, 318.8999938964844, 316.6499938964844, 341.1499938964844, 342.1499938964844, 333.1499938964844, 333.3999938964844, 333.0, 346.70001220703125, 335.79998779296875, 330.25, 326.29998779296875, 321.6499938964844, 322.54998779296875, 320.3999938964844, 320.54998779296875, 314.1000061035156, 316.75, 315.6499938964844, 316.95001220703125, 317.8999938964844, 319.70001220703125, 309.1499938964844, 308.3999938964844, 302.6499938964844, 304.8999938964844, 305.04998779296875, 297.75, 295.70001220703125, 296.04998779296875, 292.5, 291.25, 299.54998779296875, 300.6000061035156, 301.95001220703125, 308.20001220703125, 303.8500061035156, 301.29998779296875, 296.25, 295.20001220703125, 294.95001220703125, 293.54998779296875, 294.6499938964844, 293.45001220703125, 290.3500061035156, 278.1499938964844, 266.6499938964844, 264.75, 264.6000061035156, 265.04998779296875, 276.1000061035156, 285.8999938964844, 285.0, 285.6499938964844, 283.6000061035156, 285.70001220703125, 285.5, 282.95001220703125, 285.95001220703125, 283.6000061035156, 283.95001220703125, 284.3500061035156, 291.1499938964844, 297.6499938964844, 287.25, 281.20001220703125, 280.3999938964844, 284.0, 288.75, 282.1000061035156, 286.8999938964844, 286.6499938964844, 284.3999938964844, 283.0, 289.0, 295.3500061035156, 298.3999938964844, 305.6499938964844, 328.54998779296875, 345.79998779296875, 326.5, 328.1000061035156, 328.1499938964844, 323.3999938964844, 331.95001220703125, 315.79998779296875, 316.6000061035156, 316.5, 323.25, 310.79998779296875, 305.3999938964844, 296.1499938964844, 298.29998779296875, 291.45001220703125, 287.75, 311.0, 300.3500061035156, 299.5, 295.5, 306.8999938964844, 311.1499938964844, 306.8500061035156, 310.8999938964844, 312.6000061035156, 316.29998779296875, 311.20001220703125, 305.45001220703125, 296.8999938964844, 306.5, 309.1000061035156, 315.04998779296875, 282.6499938964844, 268.8500061035156, 272.45001220703125, 276.20001220703125, 278.79998779296875, 277.29998779296875, 271.3500061035156, 278.5, 282.8500061035156, 281.79998779296875, 280.04998779296875, 282.25, 280.3500061035156, 277.8999938964844, 275.45001220703125, 267.04998779296875, 246.5500030517578, 246.1999969482422, 252.8000030517578, 259.0, 255.0500030517578, 252.89999389648438, 253.75, 250.10000610351562, 248.8000030517578, 252.6999969482422, 252.39999389648438, 254.5500030517578, 263.45001220703125, 264.45001220703125, 271.04998779296875, 274.0, 277.3500061035156, 283.6499938964844, 294.54998779296875, 296.54998779296875, 301.20001220703125, 295.25, 303.6000061035156, 299.95001220703125, 295.5, 307.8500061035156, 324.1499938964844, 322.3500061035156, 316.95001220703125, 315.54998779296875, 319.25, 320.54998779296875, 324.25, 319.3999938964844, 320.75, 319.1499938964844, 321.20001220703125, 321.45001220703125, 317.8500061035156, 310.6000061035156, 326.3500061035156, 323.0, 321.95001220703125, 319.3500061035156, 314.0, 307.6000061035156, 315.45001220703125, 291.1499938964844, 307.25, 306.29998779296875, 292.5, 284.04998779296875, 273.20001220703125, 253.85000610351562, 261.6000061035156, 266.75, 266.5, 262.8999938964844, 264.95001220703125] [495.84454346 505.97668457 506.47161865 507.49160767 508.53665161\n",
      " 520.03723145 510.57748413 511.6159668  510.17538452 514.23034668\n",
      " 515.34820557 518.97180176 520.56109619 525.64892578 537.57794189\n",
      " 513.0569458  592.45239258 493.63400269 496.35858154 493.95687866\n",
      " 523.02130127 563.58184814 492.37469482 476.95935059 553.83282471\n",
      " 556.15887451 536.02905273 532.77880859 536.81768799 521.92999268\n",
      " 559.20147705 642.29919434 528.98712158 565.01782227 566.2197876\n",
      " 611.05310059 547.36242676 562.62371826 664.13110352 505.81091309\n",
      " 604.63592529 569.9083252  655.88635254 602.62426758 590.94830322\n",
      " 735.17303467 610.85626221 762.56262207 635.03277588 540.24951172\n",
      " 707.97558594 648.14611816 513.91760254 610.27233887 671.6786499\n",
      " 624.81811523 686.65319824 586.0112915  679.16748047 692.27630615\n",
      " 582.73846436 683.07904053 613.14294434 660.14361572 630.86578369\n",
      " 667.25244141 672.10479736 576.08056641 712.36694336 687.2255249\n",
      " 674.71899414 596.387146   631.84967041 639.93658447 589.85949707\n",
      " 525.83624268 642.31695557 566.24981689 383.04425049 634.48181152\n",
      " 552.93994141 627.82995605 529.5489502  352.04559326 561.7230835\n",
      " 545.56903076 622.08935547 562.85968018 489.13726807 589.80633545\n",
      " 395.00259399 516.74176025 643.96893311 408.97787476 459.53387451\n",
      " 568.28607178 381.65386963 470.89593506 358.66732788 421.31854248\n",
      " 403.98556519 525.72644043 427.08190918 477.50918579 392.10253906\n",
      " 467.58734131 482.71237183 468.07113647 471.26956177 551.57275391\n",
      " 468.09124756 423.58944702 477.59234619 456.00857544 520.8258667\n",
      " 571.15588379 523.23797607 547.23706055 502.76416016 572.99920654\n",
      " 431.35046387 540.90039062 481.97894287 308.75915527 450.67849731\n",
      " 408.39755249 467.0369873  408.76315308 477.1086731  505.86276245\n",
      " 514.92651367 480.49615479 481.3137207  442.78625488 500.57949829\n",
      " 472.212677   536.26019287 454.70993042 435.67547607 473.64996338\n",
      " 478.69464111 427.82849121 528.14434814 516.20367432 518.81604004\n",
      " 535.05310059 548.62585449 473.82769775 507.269104   469.38565063\n",
      " 466.61636353 517.52496338 438.69070435 482.10842896 484.21325684\n",
      " 558.33526611 490.07543945 463.50009155 466.78668213 501.26379395\n",
      " 484.77981567 494.73336792 456.48474121 501.30950928 473.91366577\n",
      " 489.51028442 458.47473145 462.80953979 493.52896118 496.13479614\n",
      " 429.98904419 471.39840698 478.94238281 469.98687744 467.25439453\n",
      " 472.34777832 435.99859619 509.3203125  448.77334595 259.72998047\n",
      " 378.81066895 492.82290649 470.61862183 485.81253052 491.35797119\n",
      " 386.01385498 425.91430664 520.81109619 498.45513916 535.91040039\n",
      " 519.6270752  473.41992188 467.06451416 431.30215454 473.11129761\n",
      " 486.67456055 452.27850342 501.18103027 515.50231934 447.09970093\n",
      " 519.83728027 445.04302979 486.6427002  535.67297363 522.88775635\n",
      " 505.94366455 521.78112793 509.48529053 514.18457031 554.06774902\n",
      " 546.4230957  528.33673096 526.25317383 538.63592529 543.43725586\n",
      " 587.85388184 551.8213501  542.7331543  559.72119141 538.72058105\n",
      " 541.39660645 561.96008301 550.98406982 597.56793213 599.03778076\n",
      " 537.67791748 530.7220459  573.47180176 537.2677002  566.31591797\n",
      " 568.11010742 535.14874268 565.71948242 569.3973999  566.68212891\n",
      " 579.59326172 580.98260498 580.03826904 554.01477051 559.27972412\n",
      " 576.61340332 578.5713501  558.3548584  582.18774414 557.05419922\n",
      " 572.16918945 551.3427124  585.0012207  565.92578125 581.29156494\n",
      " 563.87438965 556.43017578 560.85247803 567.03063965 554.15820312\n",
      " 569.48974609 578.66473389 568.79150391 544.23028564 562.77581787\n",
      " 551.74230957 530.74645996 538.52410889 556.0925293  520.94238281\n",
      " 547.36334229 527.07202148 537.59100342 522.50170898 559.77758789\n",
      " 517.82562256 539.50488281 525.13842773 512.71307373 528.74591064\n",
      " 527.99407959 528.53491211 480.76452637 532.37792969 567.32427979\n",
      " 519.57086182 483.52087402 509.5272522  512.42858887 508.50134277\n",
      " 494.72576904 495.44329834 497.92483521 469.41070557 532.6920166\n",
      " 482.26507568 463.50466919 490.96734619 513.63653564 478.70666504\n",
      " 502.61065674 490.71847534 493.48303223 496.91659546 517.59777832\n",
      " 542.13470459 492.82797241 513.92926025 484.98712158 478.39318848\n",
      " 492.51849365 480.93273926 496.980896   505.54278564 478.82995605\n",
      " 531.53948975 482.61761475 502.59564209 483.15429688 500.80703735\n",
      " 479.47369385 498.33538818 472.2869873  446.49456787 499.58660889\n",
      " 551.77978516 509.96130371 503.83853149 514.19610596 533.76318359\n",
      " 506.91815186 515.79901123 527.23944092 534.95324707 520.49176025\n",
      " 490.20593262 523.03155518 521.26647949 499.80487061 517.27893066\n",
      " 500.00820923 514.36547852 521.89569092 563.53515625 565.79193115\n",
      " 559.26269531 548.5289917  560.2388916  536.50506592 524.6038208\n",
      " 435.94726562 488.19622803 577.57141113 590.75817871 520.51165771\n",
      " 532.25244141 528.12738037 584.31079102 552.06860352 547.95471191\n",
      " 557.78839111 537.3671875  595.69372559 570.14489746 566.37023926\n",
      " 558.75317383 514.75091553 554.88458252 526.72216797 534.76220703\n",
      " 553.1307373  555.92767334 527.15393066 523.45935059 564.68798828\n",
      " 529.05285645 543.1192627  535.91186523 549.85339355 531.82330322\n",
      " 523.27856445 542.5378418  520.14532471 540.48535156 482.77606201\n",
      " 554.32531738 517.2097168  478.73126221 552.31384277 542.03662109\n",
      " 504.36447144 515.84069824 532.83905029 528.90112305 514.88189697\n",
      " 500.44171143 507.40917969 493.97042847 490.7147522  529.4510498\n",
      " 501.71411133 463.37399292 493.5637207  488.46588135 463.96569824\n",
      " 462.97189331 473.24206543 461.56771851 503.48809814 435.63647461\n",
      " 460.00942993 455.28039551 457.28344727 497.10668945 426.63494873\n",
      " 466.76824951 498.45214844 474.13314819 432.07434082 408.61669922\n",
      " 484.01708984 433.53625488 446.13879395 420.61703491 452.35162354\n",
      " 400.83520508 373.20568848 400.64562988 401.32095337 455.28100586\n",
      " 362.6880188  329.27584839 391.36117554 388.28890991 345.89910889\n",
      " 383.96548462 364.67382812 279.88061523 236.28494263 466.31008911\n",
      " 452.59020996 282.23284912 327.52947998 431.70361328 237.05755615\n",
      " 261.6829834  349.52185059 330.14639282 268.14331055 243.49229431\n",
      " 288.88604736 347.65380859 316.02612305 334.81243896 256.66400146\n",
      " 464.93334961 351.84951782 264.90042114 293.19958496 250.27114868\n",
      " 266.06671143 287.04199219 212.10494995 278.39837646 325.00823975\n",
      " 331.01544189 251.1441803  242.87896729 244.72297668 254.04202271\n",
      " 272.90039062 284.66394043 259.85406494 195.44934082 208.94786072\n",
      " 210.82794189 226.74525452 249.73413086 280.32403564 186.3611145\n",
      " 212.42944336 234.82144165 221.44720459 228.02807617 219.00723267\n",
      " 220.84011841 244.06983948 276.23727417 271.35543823 254.9135437\n",
      " 253.51419067 236.7300415  293.65490723 244.52990723 214.24761963\n",
      " 341.32403564 267.26751709 256.55303955 400.30963135 331.16592407\n",
      " 214.68032837 307.5274353  309.77471924 184.48522949 223.50758362\n",
      " 263.79891968 279.82940674 254.2333374  303.91381836 283.48776245\n",
      " 387.57183838 274.77593994 337.64172363 286.81213379 269.59759521\n",
      " 311.89398193 327.11077881 281.1206665  292.58395386 291.39291382\n",
      " 288.01373291 283.79516602 287.6902771  314.60702515 279.87356567\n",
      " 318.51309204 299.66217041 302.71166992 329.73654175 254.38955688\n",
      " 280.22479248 293.16387939 334.68875122 310.91433716 349.61071777\n",
      " 319.29574585 340.33755493 341.23181152 313.45910645 296.84890747\n",
      " 347.52893066 323.85977173 299.78411865 296.09738159 343.64230347\n",
      " 302.11047363 203.12988281 372.81945801 328.22814941 307.31433105\n",
      " 367.08602905 319.66299438 251.84265137 345.49942017 278.56295776\n",
      " 299.08618164 315.30560303 313.76498413 295.73547363 313.20874023\n",
      " 327.4697876  320.90466309 286.65826416 315.0213623  365.21954346\n",
      " 311.08004761 308.36721802 298.74758911 311.48025513 295.19311523\n",
      " 315.0065918  307.69335938 301.82373047 304.51449585 293.09112549\n",
      " 308.69146729 306.58746338 318.32806396 313.33563232 314.82958984\n",
      " 305.68597412 306.37176514 326.4864502  308.52593994 291.5880127\n",
      " 310.81622314 296.64801025 296.90380859 291.71450806 314.26690674\n",
      " 281.77053833 316.8861084  322.57821655 324.67462158 334.06369019\n",
      " 294.24978638 344.46160889 304.95013428 298.69671631 320.42657471\n",
      " 310.36282349 319.14129639 292.53451538 322.49636841 344.4197998\n",
      " 361.3614502  337.03237915 348.77947998 335.32391357 314.94918823\n",
      " 351.51855469 342.50305176 338.4664917  330.36911011 326.42364502\n",
      " 343.64959717 349.80355835 362.61138916 352.63024902 316.46514893\n",
      " 340.73861694 349.32775879 349.77478027 387.88449097 331.26638794\n",
      " 381.05612183 337.56130981 375.21279907 337.79882812 361.5302124\n",
      " 304.44665527 372.8420105  338.75463867 385.63165283 371.77658081\n",
      " 347.66494751 423.43548584 375.77789307 374.5428772  379.99847412\n",
      " 363.31555176 370.27520752 371.37643433 356.87689209 349.3605957\n",
      " 354.34854126 388.46932983 402.90319824 371.61581421 395.56695557\n",
      " 405.41796875 358.00256348 356.12677002 359.25909424 392.81210327\n",
      " 376.84558105 374.87319946 345.30981445 351.15139771 405.90045166\n",
      " 378.55206299 383.22128296 377.31140137 384.25323486 368.25177002\n",
      " 354.99020386 351.02133179 310.23242188 359.60864258 364.55682373\n",
      " 382.36608887 363.80667114 352.51303101 358.11746216 312.65087891\n",
      " 350.83029175 349.66973877 339.78512573 342.58450317 416.31539917\n",
      " 336.67416382 328.54980469 328.30340576 318.37136841 340.23773193\n",
      " 345.87362671 343.15866089 347.27709961 349.5        335.3503418\n",
      " 331.26733398 323.61856079 330.62927246 323.81359863 343.00439453\n",
      " 331.13137817 320.56158447 341.79522705 319.49707031 339.91488647\n",
      " 344.41714478 339.94244385 328.7088623  330.56008911 319.30349731\n",
      " 339.29483032 334.14746094 302.38763428 345.84521484 313.13006592\n",
      " 327.8263855  357.77767944 282.44873047 333.74884033 345.60571289\n",
      " 328.26733398 332.33752441 331.94317627 334.60092163 361.59844971\n",
      " 336.1137085  308.70654297 330.0430603  327.6741333  316.8555603\n",
      " 334.78250122 312.49639893 310.75750732 319.08111572 309.84939575\n",
      " 316.20059204 325.12039185 313.50390625 315.918396   325.01416016\n",
      " 318.12213135 319.12121582 320.71478271 307.3817749  316.15661621\n",
      " 319.17626953 320.77526855 312.45355225 303.7802124  319.9503479\n",
      " 310.40927124 310.72943115 310.8934021  307.7321167  305.60708618\n",
      " 308.70825195 303.52493286 303.24179077 310.73092651 315.25006104\n",
      " 307.07525635 309.13677979 296.32904053 333.82510376 324.2300415\n",
      " 312.60983276 300.82626343 331.42260742 318.59454346 326.41064453\n",
      " 321.63284302 304.39248657 301.53527832 322.20996094 332.02929688\n",
      " 301.68225098 319.30773926 327.17144775 296.26483154 308.05505371\n",
      " 293.86618042 323.51629639 313.73693848 306.29949951 296.78668213\n",
      " 288.14666748 318.10223389 299.89294434 292.94454956 300.74453735\n",
      " 303.08154297 285.47235107 329.69372559 293.60238647 320.73086548\n",
      " 292.33413696 290.07965088 278.87509155 287.89788818 319.31219482\n",
      " 296.7510376  297.11526489 301.56848145 289.01989746 301.09494019\n",
      " 291.61547852 295.62295532 295.06216431 296.74102783 301.06530762\n",
      " 297.96313477 283.42175293 284.46380615 283.7722168  301.96511841\n",
      " 281.93255615 287.2883606  290.78399658 309.8097229  291.40939331\n",
      " 292.36990356 301.74667358 296.96911621 297.13430786 306.86679077\n",
      " 308.86968994 314.32476807 312.29266357 287.26696777 299.32971191\n",
      " 282.300354   298.62561035 305.82788086 301.01052856 290.02072144\n",
      " 311.5050354  289.33148193 300.84844971 302.56842041 308.35736084\n",
      " 293.59692383 300.65560913 300.13012695 304.08126831 295.5322876\n",
      " 296.06246948 302.58175659 291.1882019  291.81491089 300.83700562\n",
      " 294.38128662 306.00320435 292.68365479 288.35287476 295.24407959\n",
      " 305.7835083  305.01187134 303.5508728  292.95950317 289.04208374\n",
      " 286.12573242 293.79116821 290.37887573 298.48614502 278.75341797\n",
      " 283.10888672 287.44720459 271.92544556 297.84832764 281.85577393\n",
      " 296.41848755 256.05413818 291.00927734 305.59765625 287.18157959\n",
      " 282.34893799 282.48959351 295.30072021 287.87161255 276.16915894\n",
      " 295.73999023 270.11782837 297.1854248  276.09320068 280.62567139\n",
      " 273.45257568 291.28042603 282.27767944 278.7741394  287.33807373\n",
      " 277.6968689  291.86010742 277.84872437 285.09277344 283.77194214\n",
      " 283.75680542 294.85629272 280.06842041 293.58319092 288.91741943\n",
      " 287.19592285 299.19830322 294.3034668  287.85324097 269.91226196\n",
      " 285.22839355 290.96414185 293.64736938 298.63861084 292.72607422\n",
      " 284.28497314 280.52502441 315.29019165 270.02993774 287.50814819\n",
      " 299.5262146  282.61898804 281.53726196 297.41522217]\n",
      "Processing completed for BANDHANBNK.NS\n"
     ]
    }
   ],
   "source": [
    "list_dir = 'data_collection/stocks_list'\n",
    "list_prefix = \"ind_nifty\"\n",
    "list_suffix = \"list.csv\"\n",
    "save_dir = 'data_collection/ohlcv_data'\n",
    "save_prefix = \"ohlcv_\"\n",
    "save_suffix = \".csv\"\n",
    "resultfile = \"results/baseline_lstm_vae.csv\"\n",
    "cap_n_stocks = 10\n",
    "\n",
    "skip_till = \"HEROMOTOCO.NS\"  # Set to None if nothing is to be skipped. \n",
    "results = []\n",
    "start = skip_till is None\n",
    "\n",
    "use_vae = True\n",
    "vae_latent_dims = 2 # Must be > half of features.\n",
    "\n",
    "len_window = 10\n",
    "n_tr = 60\n",
    "n_ts = 1\n",
    "\n",
    "for f in os.listdir(list_dir):\n",
    "    if f.startswith(list_prefix) and f.endswith(list_suffix):\n",
    "        savefile = os.path.join(save_dir, save_prefix+f[9:-8]+save_suffix)\n",
    "        listfile = os.path.join(list_dir, f)\n",
    "        p = pd.read_csv(listfile)\n",
    "        symbols = list(p['Symbol'].values + '.NS')\n",
    "        if cap_n_stocks <= 0:\n",
    "            break\n",
    "        for symbol in symbols:\n",
    "            if not start:\n",
    "                if symbol != skip_till:\n",
    "                    continue \n",
    "                else:\n",
    "                    start = True \n",
    "                    continue  # skip_till is skip inclusive i.e. stock = skip_till will be skipped.\n",
    "\n",
    "            cap_n_stocks -= 1\n",
    "            if cap_n_stocks <= 0:\n",
    "                break\n",
    "\n",
    "            df = get_indi_df(symbol, ohlcvfile=savefile, start_date=\"2017-01-01\")\n",
    "            # df = get_all_ta_features(df)\n",
    "            drop_columns = ['Date', 'Adj Close', 'Volume']\n",
    "            df.drop(drop_columns, axis=1, inplace=True)\n",
    "            move_dir_target, cls_target = get_labels(df['Close'])\n",
    "            df = df.iloc[:-1]\n",
    "            unflattened_df_cls = df['Close']\n",
    "\n",
    "            min_max_scaler = MinMaxScaler()\n",
    "            df = min_max_scaler.fit_transform(df)  # Data Leakage! To be fixed.\n",
    "            if use_vae:\n",
    "                vae = VariationalAutoencoder(ip_dim=prod(df.shape)//df.shape[0], latent_dims=vae_latent_dims)\n",
    "                vae = train_vae(vae, torch.Tensor(df), epochs=50)\n",
    "                df = vae.encoder(torch.Tensor(df)).detach().numpy()\n",
    "                print(pd.DataFrame(df).head())\n",
    "\n",
    "            swdf10 = (sliding_window_view(df, (10, df.shape[1]))).squeeze()\n",
    "            cls_target10 = cls_target.iloc[(10 - 1):-1]\n",
    "            # df10 = slide_and_flatten(df, window_len=10)\n",
    "            # df10 = pd.DataFrame(df10, index=df.index[9:])\n",
    "            # df30 = slide_and_flatten(df, window_len=30)\n",
    "            # df30 = pd.DataFrame(df30, index=df.index[29:])\n",
    "            # df60 = slide_and_flatten(df, window_len=60)\n",
    "            # df60 = pd.DataFrame(df60, index=df.index[59:])\n",
    "            # print(swdf10.shape, cls_target10.shape)\n",
    "            #           \n",
    "            model = LSTMBaseline(swdf10.shape[2], 64)\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=0.1)\n",
    "            loss_fn = nn.MSELoss()\n",
    "            post_processor = (add_closing_price, {'cls_price':unflattened_df_cls.iloc[(len_window-1):len(unflattened_df_cls)-(n_tr+n_ts)]})\n",
    "            result = sliding_window_cv_torch(swdf10, cls_target10, model, optimizer, loss_fn, n_tr=n_tr, n_ts=n_ts, \n",
    "            scorers=[mean_squared_error,mean_absolute_percentage_error, r2_score], comment=\"lstm_woattn_ohlcv_{}\".format(symbol), post_processor=None)\n",
    "            results.append(result)\n",
    "            if resultfile is not None:\n",
    "                file_exists = os.path.isfile(resultfile)\n",
    "\n",
    "                with open(resultfile, 'a', newline='') as f:\n",
    "                    writer = csv.DictWriter(f, fieldnames=results[0].keys(), delimiter=',', lineterminator='\\n')\n",
    "\n",
    "                    if not file_exists:\n",
    "                        writer.writeheader()  # file doesn't exist yet, write a header\n",
    "\n",
    "                    writer.writerows(results)\n",
    "\n",
    "            print(\"Processing completed for {}\".format(symbol))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d956a392b67227f401c129a901ef5c98887812674686cb9105f1c9b415cc849"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}