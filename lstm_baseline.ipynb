{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocessing.wrangling import get_indi_df, get_labels, slide_and_flatten\n",
    "from preprocessing.extract_features import get_all_ta_features, get_wavelet_coeffs\n",
    "from evaluation.eval import sliding_window_cv_regression, batch_test_swcv_regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import datetime\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_closing_price(y, cls_price):\n",
    "    return y + cls_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersistanceModel:\n",
    "    def __init__(self, persist_colname='Close'):\n",
    "        self.persist_colname = persist_colname\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"PersistanceModel(persist_colname={})\".format(self.persist_colname)\n",
    "\n",
    "    def fit(self, Xtr, ytr):\n",
    "        pass\n",
    "\n",
    "    def predict(self, Xts):\n",
    "        return Xts.loc[:, self.persist_colname]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_cv_torch(swdf, y, model, optimizer, loss_fn, n_tr, n_ts=1, scorers=[], comment=\"\", post_processor=None):\n",
    "    assert len(swdf) == len(y), \"Length of X ([]) must match that of y ([]).\".format(len(X), len(y))\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    y_pred = []\n",
    "    y_target = []\n",
    "    agg_results = {}\n",
    "    if post_processor is not None:\n",
    "        post_processor_f, post_processor_args = post_processor[0], post_processor[1]\n",
    "        \n",
    "\n",
    "    for i_tr_start in range(0, len(swdf)-(n_tr+n_ts)):\n",
    "        # The last i_ts_end should be len(X).\n",
    "        # i_ts_end = i_ts_start + n_ts\n",
    "        # Now, i_tr_end = i_ts_start\n",
    "        # So, i_tr_start = i_ts_start - n_tr\n",
    "        # But, i_ts_start = i_ts_end - n_ts\n",
    "        # Thus, i_tr_start = i_ts_end - n_tr - n_ts\n",
    "        # Hence, last i_tr_start = len(X) - (n_tr + n_ts)\n",
    "\n",
    "        i_tr_end = i_ts_start = i_tr_start + n_tr \n",
    "        i_ts_end = i_ts_start + n_ts \n",
    "\n",
    "        Xtr, Xts = swdf[i_tr_start:i_tr_end, :, :], swdf[i_ts_start:i_ts_end, :, :]\n",
    "        ytr, yts = y[i_tr_start:i_tr_end].to_numpy(), y[i_ts_start:i_ts_end].to_numpy()\n",
    "        Xtr, Xts, ytr, yts = torch.Tensor(Xtr), torch.Tensor(Xts), torch.Tensor(ytr), torch.Tensor(yts)\n",
    "        Xtr, Xts, ytr, yts = Xtr.float().to(device), Xts.float().to(device), ytr.float().to(device), yts.float().to(device)\n",
    "        \n",
    "        model.to(device)\n",
    "        \n",
    "        epochs = 10\n",
    "        for e in range(epochs):\n",
    "            model.train()\n",
    "            pred = model(Xtr)\n",
    "            loss = loss_fn(pred, ytr)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(Xts)\n",
    "            mape = torch.mean((torch.abs((yts - pred) / yts)) * 100)\n",
    "            if device == \"cuda\":\n",
    "                pred = pred.detach()\n",
    "                yts = yts.detach()\n",
    "\n",
    "            if len(pred.shape) == 0:\n",
    "                pred = pred.unsqueeze(0)\n",
    "            if pred.shape[0] == 1:\n",
    "                y_pred.append(pred.item())\n",
    "                y_target.append(yts.item())\n",
    "            else:\n",
    "                y_pred, yts = list(y_pred), list(yts)\n",
    "                y_pred.extend(y_pred)\n",
    "                y_target.extend(yts)\n",
    "\n",
    "    # print(len(y_target), len(y_pred))\n",
    "    # print(y_pred, y_target)\n",
    "    if len(y_pred) > 1:\n",
    "        y_pred = np.squeeze(y_pred)\n",
    "\n",
    "    if post_processor is not None:\n",
    "        y_pred = post_processor_f(y_pred, **post_processor_args)\n",
    "        y_target = post_processor_f(y_target, **post_processor_args)\n",
    "\n",
    "    agg_results['time'] = datetime.datetime.now()\n",
    "    agg_results['model'] = str(model)\n",
    "    agg_results['comment'] = comment\n",
    "    for scorer in scorers:\n",
    "        agg_results[scorer.__name__] = scorer(y_target, y_pred)\n",
    "\n",
    "    return agg_results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBaseline(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMBaseline, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.dense = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, swdf):\n",
    "        _, hncn = self.lstm(swdf)\n",
    "        hn, cn = hncn\n",
    "        hn = hn.squeeze()\n",
    "        op = self.dense(hn)\n",
    "        return op.squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n",
      "Processing completed for FEDERALBNK.NS\n",
      "Using cpu device\n",
      "Processing completed for HDFCBANK.NS\n",
      "Using cpu device\n",
      "Processing completed for ICICIBANK.NS\n",
      "Using cpu device\n",
      "Processing completed for IDFCFIRSTB.NS\n",
      "Using cpu device\n",
      "Processing completed for INDUSINDBK.NS\n",
      "Using cpu device\n",
      "Processing completed for KOTAKBANK.NS\n",
      "Using cpu device\n",
      "Processing completed for PNB.NS\n",
      "Using cpu device\n",
      "Processing completed for RBLBANK.NS\n",
      "Using cpu device\n",
      "Processing completed for SBIN.NS\n"
     ]
    }
   ],
   "source": [
    "list_dir = 'data_collection/stocks_list'\n",
    "list_prefix = \"ind_nifty\"\n",
    "list_suffix = \"list.csv\"\n",
    "save_dir = 'data_collection/ohlcv_data'\n",
    "save_prefix = \"ohlcv_\"\n",
    "save_suffix = \".csv\"\n",
    "resultfile = \"results/baseline_lstm.csv\"\n",
    "cap_n_stocks = float('inf')  # Set to float('inf') for bypassing cap.\n",
    "\n",
    "skip_till = \"SBIN.NS\"  # Set to None if nothing is to be skipped. \n",
    "results = []\n",
    "start = skip_till is None\n",
    "\n",
    "for f in os.listdir(list_dir):\n",
    "    if f.startswith(list_prefix) and f.endswith(list_suffix):\n",
    "        savefile = os.path.join(save_dir, save_prefix+f[9:-8]+save_suffix)\n",
    "        listfile = os.path.join(list_dir, f)\n",
    "        p = pd.read_csv(listfile)\n",
    "        symbols = list(p['Symbol'].values + '.NS')\n",
    "        if cap_n_stocks <= 0:\n",
    "            break\n",
    "        for symbol in symbols:\n",
    "            if not start:\n",
    "                if symbol != skip_till:\n",
    "                    continue \n",
    "                else:\n",
    "                    start = True \n",
    "                    continue  # skip_till is skip inclusive i.e. stock = skip_till will be skipped.\n",
    "\n",
    "            cap_n_stocks -= 1\n",
    "            if cap_n_stocks <= 0:\n",
    "                break\n",
    "\n",
    "            df = get_indi_df(symbol, ohlcvfile=savefile, start_date=\"2017-01-01\")\n",
    "            # df = get_all_ta_features(df)\n",
    "            drop_columns = ['Date', 'Adj Close']\n",
    "            df.drop(drop_columns, axis=1, inplace=True)\n",
    "            move_dir_target, cls_target = get_labels(df['Close'])\n",
    "            df = df.iloc[:-1]\n",
    "            swdf10 = (sliding_window_view(df, (10, df.shape[1]))).squeeze()\n",
    "            cls_target10 = cls_target.iloc[(10 - 1):-1]\n",
    "            # df10 = slide_and_flatten(df, window_len=10)\n",
    "            # df10 = pd.DataFrame(df10, index=df.index[9:])\n",
    "            # df30 = slide_and_flatten(df, window_len=30)\n",
    "            # df30 = pd.DataFrame(df30, index=df.index[29:])\n",
    "            # df60 = slide_and_flatten(df, window_len=60)\n",
    "            # df60 = pd.DataFrame(df60, index=df.index[59:])\n",
    "            \n",
    "            # print(swdf10.shape, cls_target10.shape)\n",
    "            model = LSTMBaseline(swdf10.shape[2], 64)\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=0.1)\n",
    "            loss_fn = nn.MSELoss()\n",
    "            result = sliding_window_cv_torch(swdf10, cls_target10, model, optimizer, loss_fn, n_tr=60, n_ts=1, \n",
    "            scorers=[mean_squared_error,mean_absolute_percentage_error, r2_score], comment=\"lstm_woattn_ohlcv_{}\".format(symbol), post_processor=None)\n",
    "            results.append(result)\n",
    "            if resultfile is not None:\n",
    "                file_exists = os.path.isfile(resultfile)\n",
    "\n",
    "                with open(resultfile, 'a', newline='') as f:\n",
    "                    writer = csv.DictWriter(f, fieldnames=results[0].keys(), delimiter=',', lineterminator='\\n')\n",
    "\n",
    "                    if not file_exists:\n",
    "                        writer.writeheader()  # file doesn't exist yet, write a header\n",
    "\n",
    "                    writer.writerows(results)\n",
    "\n",
    "            print(\"Processing completed for {}\".format(symbol))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d956a392b67227f401c129a901ef5c98887812674686cb9105f1c9b415cc849"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}