{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view as sliding_window_view\n",
    "import pickle\n",
    "import datetime\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSEDataset(Dataset):\n",
    "    def __init__(self, ohlcv_dir, target_ticker, target_ticker_file, len_window, len_corr_traceback, nP, nN, \n",
    "    keep_tickers=None, ohlcv_prefix='', ohlcv_sufix='', ohlcv_files=None, start_date=None, end_date=None,\n",
    "    target_feat='c', keep_feat='ohlcva', normalize='min-max'):\n",
    "\n",
    "        feat_name_map = {\n",
    "            'o' : 'Open', \n",
    "            'h' : 'High', \n",
    "            'l' : 'Low', \n",
    "            'c' : 'Close', \n",
    "            'v' : 'Volume',\n",
    "            'a' : 'Adj Close'\n",
    "        }\n",
    "\n",
    "        self.len_window = len_window\n",
    "        self.len_corr_traceback = len_corr_traceback\n",
    "        self.nP, self.nN = nP, nN\n",
    "        self.target_feat = target_feat\n",
    "        self.keep_feat = keep_feat\n",
    "        self.start_date, self.end_date = start_date, end_date\n",
    "\n",
    "        if ohlcv_files is not None:\n",
    "            ohlcv_files = set(ohlcv_files)\n",
    "        \n",
    "        if keep_tickers is not None:\n",
    "            keep_tickers = set(keep_tickers)\n",
    "    \n",
    "        df = pd.read_csv(os.path.join(ohlcv_dir, target_ticker_file))\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "        if start_date is not None:\n",
    "            start_mask =  df['Date'] >= datetime.datetime.fromisoformat(start_date)\n",
    "            i_start = start_mask[start_mask].index.min()\n",
    "        else:\n",
    "            i_start = 0\n",
    "        \n",
    "        if end_date is not None:\n",
    "            end_mask =  df['Date'] > datetime.datetime.fromisoformat(end_date)\n",
    "            i_end = end_mask[end_mask].index.min()\n",
    "        else:\n",
    "            i_end = len(df)\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.set_index(['Date', 'Ticker'], inplace=True)\n",
    "        df = df.iloc[i_start:i_end]\n",
    "        \n",
    "        self.mainstream_df = df.loc[:, target_ticker]\n",
    "        self.df = df.drop(target_ticker, axis=1)\n",
    "        \n",
    "        if ohlcv_files is not None and target_ticker_file not in ohlcv_files:\n",
    "            self.df = pd.DataFrame(columns=df.columns)\n",
    "            \n",
    "        for f in os.listdir(ohlcv_dir):\n",
    "            if f.startswith(ohlcv_prefix) and f.endswith(ohlcv_sufix) and (ohlcv_files is None or f in ohlcv_files):\n",
    "                if f == target_ticker_file:\n",
    "                    continue\n",
    "                else:\n",
    "                    temp_df = pd.read_csv(os.path.join(ohlcv_dir, f))\n",
    "                    temp_df.reset_index(drop=True, inplace=True)\n",
    "                    temp_df['Date'] = pd.to_datetime(temp_df['Date'])\n",
    "                    self.df = pd.merge(self.df.reset_index(), temp_df, on=['Date', 'Ticker'],\n",
    "                    how='inner', suffixes=('', '_y')).set_index(['Date', 'Ticker'])\n",
    "                    self.df.drop(self.df.filter(regex='_y$').columns.tolist(), axis=1, inplace=True)\n",
    "                    \n",
    "        if keep_tickers is not None:\n",
    "            for c in self.df:\n",
    "                if c not in keep_tickers:\n",
    "                    self.df.drop(c, axis=1, inplace=True)\n",
    "\n",
    "        \n",
    "        self.df = self.df.pivot_table(index='Date', columns='Ticker')\n",
    "        self.df.columns = self.df.columns.map('_'.join)\n",
    "        if isinstance(self.mainstream_df, pd.Series):\n",
    "            self.mainstream_df = pd.DataFrame(self.mainstream_df)\n",
    "        \n",
    "        self.mainstream_df = self.mainstream_df.pivot_table(index='Date', columns='Ticker')\n",
    "        self.mainstream_df.columns = self.mainstream_df.columns.map('_'.join)\n",
    "\n",
    "        target_feat_name = \"{}_{}\".format(target_ticker, feat_name_map[target_feat])\n",
    "        \n",
    "        self.unshifted_target = self.mainstream_df.loc[:, target_feat_name]\n",
    "        self.target = self.unshifted_target.shift(periods=-1).iloc[:-1]\n",
    "\n",
    "        # Min-max normalizing.\n",
    "        if normalize == 'min-max':\n",
    "            self.target = (self.target - self.target.min())/(self.target.max() - self.target.min())\n",
    "            self.unshifted_target = (self.unshifted_target - self.unshifted_target.min())/(self.unshifted_target.max() - self.unshifted_target.min())\n",
    "\n",
    "        # To account for absence of target for last row.\n",
    "        self.df = self.df.iloc[:-1, :]  \n",
    "        self.mainstream_df = self.mainstream_df.iloc[:-1, :]\n",
    "        \n",
    "        # Min-max Normalizing\n",
    "        if normalize == 'min-max':\n",
    "            self.df = (self.df-self.df.min())/(self.df.max()-self.df.min())\n",
    "            self.mainstream_df = (self.mainstream_df-self.mainstream_df.min())/(self.mainstream_df.max()-self.mainstream_df.min())\n",
    "        \n",
    "        drop_features = set(feat_name_map.keys()).difference({feat for feat in keep_feat})\n",
    "        for feat in drop_features:\n",
    "            self.df.drop(self.df.filter(regex='_{}$'.format(feat_name_map[feat])).columns.tolist(), axis=1, inplace=True)\n",
    "            self.mainstream_df.drop(self.mainstream_df.filter(regex='_{}$'.format(feat_name_map[feat])).columns.tolist(), axis=1, inplace=True)\n",
    "\n",
    "        # For i_end, data of [i_end - (len_corr_traceback) : i_end] (py notation)\n",
    "        # is needed to calculate correlation, basis on which data of \n",
    "        # [i_end - (len_window) : i_end] (py notation) must be sent.\n",
    "        # i_end is excluded, so last i_end should be len(self.df)\n",
    "        \n",
    "        self.min_chosen_p = float('inf')\n",
    "        self.max_chosen_n = float('-inf')\n",
    "        self.swdf = []\n",
    "        for i_end in range(len_corr_traceback, len(self.df)+1):\n",
    "            self.swdf.append(self.get_high_corr(self.unshifted_target.iloc[i_end-len_corr_traceback:i_end], \n",
    "            self.df.iloc[i_end-len_corr_traceback:i_end, :], len_window, nP, nN))\n",
    "            \n",
    "        # self.swdf = np.array(self.swdf).reshape(len(self.swdf), self.len_window, -1)\n",
    "        self.swdf = np.array(self.swdf)\n",
    "        self.swdf = pd.DataFrame(self.swdf).fillna(method='ffill').to_numpy()\n",
    "        \n",
    "        # if earlier self.df.shape was (6(n+1), c), it should now be\n",
    "        # (n, c), mainstream_df.shape and index_data_df should be (n, 1) and swdf.shape\n",
    "        # should be (n-lct+1, lw*(nP+nN)).\n",
    "\n",
    "        # For index 0, \n",
    "        # swdf[0], mainstream_df[lct-lw : lct] flattend (py notation)\n",
    "        # index_data_df[lct-lw : lct] flattened (py notation) should be accessed.\n",
    "\n",
    "        # For index i < len(swdf), \n",
    "        # swdf[i], mainstream_df[lct-lw + i: lct + i] flattend (py notation)\n",
    "        # index_data_df[lct-lw + i : lct + i] flattened (py notation) should be accessed.\n",
    "        # Correct. Continue from here. \n",
    "\n",
    "        # The index data used is for a single index.\n",
    "        self.index_data_df = pd.read_csv(\"data_collection/NIFTY 50.csv\")\n",
    "        self.index_data_df['Date'] = pd.to_datetime(self.index_data_df['Date'])\n",
    "        self.index_data_df.rename(columns={'SharesTraded' : 'Volume'}, inplace=True)\n",
    "        self.index_data_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "        if start_date is not None:\n",
    "            start_mask =  self.index_data_df['Date'] >= datetime.datetime.fromisoformat(start_date)\n",
    "            i_start = start_mask[start_mask].index.min()\n",
    "        else:\n",
    "            i_start = 0\n",
    "\n",
    "        if end_date is not None:\n",
    "            end_mask =  self.index_data_df['Date'] > datetime.datetime.fromisoformat(end_date)\n",
    "            i_end = end_mask[end_mask].index.min()\n",
    "        else:\n",
    "            i_end = len(self.index_data_df)\n",
    "\n",
    "        self.index_data_df.reset_index(drop=True, inplace=True)\n",
    "        self.index_data_df.set_index(['Date'], inplace=True)\n",
    "        self.index_data_df = self.index_data_df.iloc[i_start:i_end]\n",
    "\n",
    "        # Min-max normalizing.\n",
    "        if normalize == 'min-max':\n",
    "            self.index_data_df=(self.index_data_df-self.index_data_df.min())/(self.index_data_df.max()-self.index_data_df.min())\n",
    "\n",
    "        self.index_data_df = pd.DataFrame(self.index_data_df.loc[:, 'Close'])\n",
    "        self.index_data_df = self.index_data_df.reset_index().merge(\n",
    "    self.df.reset_index()['Date'], how='inner', on='Date').set_index('Date')\n",
    "\n",
    "        print(\"Dataset created for {}\".format(target_ticker))\n",
    "\n",
    "    def get_high_corr(self, target: pd.Series, candidates: pd.DataFrame, len_window, nP, nN):\n",
    "        corr = candidates.corrwith(target)\n",
    "        p_best = corr.nlargest(nP)\n",
    "        n_best = corr.nsmallest(nN)\n",
    "        self.min_chosen_p = min(self.min_chosen_p, min(p_best))\n",
    "        self.max_chosen_n = max(self.max_chosen_n, max(n_best))\n",
    "        assert not p_best.isna().any()\n",
    "        assert not n_best.isna().any()        \n",
    "        newrow = candidates.iloc[-len_window:, candidates.columns.get_indexer(p_best.index)].melt()['value'].tolist()\n",
    "        newrow.extend(candidates.iloc[-len_window:, candidates.columns.get_indexer(n_best.index)].melt()['value'].tolist())\n",
    "        return newrow\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.swdf)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For index i < len(swdf), \n",
    "        # swdf[i], mainstream_df[lct-lw + i: lct + i] flattend (py notation)\n",
    "        # index_data_df[lct-lw + i : lct + i] flattened (py notation) should be accessed.\n",
    "        \n",
    "        return (self.swdf[idx, :].reshape(self.len_window, -1), \n",
    "        self.mainstream_df.iloc[self.len_corr_traceback-self.len_window+idx : self.len_corr_traceback+idx].to_numpy(), \n",
    "        self.index_data_df.iloc[self.len_corr_traceback-self.len_window+idx : self.len_corr_traceback+idx].to_numpy(),\n",
    "        self.target[self.len_corr_traceback+idx-1]\n",
    "        )\n",
    "\n",
    "\n",
    "def save_NSEDataset(dataset, opfile):\n",
    "    with open(opfile, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "def load_NSEDataset(ipfile):\n",
    "    PICKLE_PROTOCOL = 4\n",
    "    with open(ipfile, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " class MiLSTM(nn.Module):\n",
    "    def __init__(self, input_sz: int, hidden_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        self.p_size = input_sz \n",
    "        self.n_size = input_sz \n",
    "        self.index_size = input_sz\n",
    "        p_sz, n_sz, index_sz = self.p_size, self.n_size, self.index_size\n",
    "\n",
    "        #f_t\n",
    "        self.Wfh = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wfy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bf = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #o_t\n",
    "        self.Woh = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Woy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bo = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #c_t\n",
    "        self.Wch = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wcy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bc = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #c_pt\n",
    "        self.Wcph = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wcpp = nn.Parameter(torch.Tensor(p_sz, hidden_sz))\n",
    "        self.bcp = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        #c_nt\n",
    "        self.Wcnh = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wcnn = nn.Parameter(torch.Tensor(n_sz, hidden_sz))\n",
    "        self.bcn = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #c_it\n",
    "        self.Wcih = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wcii = nn.Parameter(torch.Tensor(index_sz, hidden_sz))\n",
    "        self.bci = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        #i_t\n",
    "        self.Wih = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wiy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bi = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #i_pt\n",
    "        self.Wiph = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wipy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bip = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        #c_nt\n",
    "        self.Winh = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Winy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bin = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #c_it\n",
    "        self.Wiih = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wiiy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bii = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        #attn\n",
    "        self.alpha_t = nn.Parameter(torch.Tensor(1))\n",
    "        self.alpha_pt = nn.Parameter(torch.Tensor(1))\n",
    "        self.alpha_nt = nn.Parameter(torch.Tensor(1))\n",
    "        self.alpha_it = nn.Parameter(torch.Tensor(1))\n",
    "        self.Wattn = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.ba = nn.Parameter(torch.Tensor(1))\n",
    "        self.bap = nn.Parameter(torch.Tensor(1))\n",
    "        self.ban = nn.Parameter(torch.Tensor(1))\n",
    "        self.bai = nn.Parameter(torch.Tensor(1))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        #c_pt\n",
    "        nn.init.zeros_(self.Wcph)\n",
    "        nn.init.zeros_(self.Wcpp)\n",
    "        nn.init.zeros_(self.bcp)\n",
    "        \n",
    "        #c_nt\n",
    "        nn.init.zeros_(self.Wcnh)\n",
    "        nn.init.zeros_(self.Wcnn)\n",
    "        nn.init.zeros_(self.bcn)\n",
    "        \n",
    "        #c_it\n",
    "        nn.init.zeros_(self.Wcih)\n",
    "        nn.init.zeros_(self.Wcii)\n",
    "        nn.init.zeros_(self.bci)\n",
    "        \n",
    "\n",
    "    def forward(self, y_tilde, p_tilde, n_tilde, index_tilde, init_stats=None):\n",
    "        batch_size, win_len, _ = y_tilde.shape\n",
    "        hidden_seqs = []\n",
    "        cell_states = []\n",
    "\n",
    "        if init_stats is None:\n",
    "            h_t, cell_t = (torch.zeros(batch_size, self.hidden_size).to(y_tilde.device), \n",
    "                        torch.zeros(batch_size, self.hidden_size).to(y_tilde.device))\n",
    "        else:\n",
    "            h_t, cell_t = init_states \n",
    "\n",
    "        \n",
    "        for t in range(win_len):\n",
    "            y_t = y_tilde[:, t, :]\n",
    "            p_t = p_tilde[:, t, :]\n",
    "            n_t = n_tilde[:, t, :]\n",
    "            index_t = index_tilde[:, t, :]\n",
    "\n",
    "            f_t = torch.sigmoid(y_t @ self.Wfy + h_t @ self.Wfh + self.bf)\n",
    "            o_t = torch.sigmoid(y_t @ self.Woy + h_t @ self.Woh + self.bo)\n",
    "            c_t = torch.tanh(y_t @ self.Wcy + h_t @ self.Wch + self.bc)\n",
    "            c_pt = torch.tanh(p_t @ self.Wcpp + h_t @ self.Wcph + self.bcp)\n",
    "            c_nt = torch.tanh(n_t @ self.Wcnn + h_t @ self.Wcnh + self.bcn)\n",
    "            c_it = torch.tanh(index_t @ self.Wcii + h_t @ self.Wcph + self.bci)\n",
    "\n",
    "            i_t = torch.sigmoid(y_t @ self.Wiy + h_t @ self.Wih + self.bi)\n",
    "            i_pt = torch.sigmoid(y_t @ self.Wipy + h_t @ self.Wiph + self.bip)\n",
    "            i_nt = torch.sigmoid(y_t @ self.Winy + h_t @ self.Winh + self.bin)\n",
    "            i_it  = torch.sigmoid(y_t @ self.Wiiy + h_t @ self.Wiih + self.bii)\n",
    "\n",
    "            l_t = torch.mul(c_t, i_t)\n",
    "            l_pt = torch.mul(c_pt, i_pt)\n",
    "            l_nt = torch.mul(c_nt, i_nt)\n",
    "            l_it = torch.mul(c_it, i_it)\n",
    "           \n",
    "            u_t = torch.mul(l_t @ self.Wattn, cell_t).sum(dim=1)\n",
    "            u_pt = torch.mul(l_pt @ self.Wattn, cell_t).sum(dim=1)\n",
    "            u_nt = torch.mul(l_nt @ self.Wattn, cell_t).sum(dim=1)\n",
    "            u_it = torch.mul(l_it @ self.Wattn, cell_t).sum(dim=1)\n",
    "\n",
    "            alphas = torch.stack((u_t, u_pt, u_nt, u_it), dim=1)\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            probs = softmax(alphas)\n",
    "            alpha_t, alpha_pt, alpha_nt, alpha_it = probs[:, 0], probs[:, 1], probs[:, 2], probs[:, 3]\n",
    "            \n",
    "            L_t = self.alpha_t*l_t + self.alpha_pt*l_pt + self.alpha_nt*l_nt + self.alpha_it*l_it\n",
    "\n",
    "            cell_t = torch.mul(cell_t, f_t) + L_t\n",
    "            h_t = torch.mul(torch.tanh(cell_t), o_t)\n",
    "            \n",
    "            hidden_seqs.append(h_t)\n",
    "            cell_states.append(cell_t)\n",
    "\n",
    "        hidden_seqs = torch.stack(hidden_seqs)\n",
    "        hidden_seqs = hidden_seqs.transpose(0, 1).contiguous()\n",
    "        return hidden_seqs, (h_t, cell_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, keep_features, hidden_sz1, hidden_sz2, hidden_sz_lin1, hidden_sz_lin2, hidden_sz_lin3, nP, nN, lstm1_layers=1, lstm1_dropout=0):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden_sz1 = hidden_sz1\n",
    "        self.hidden_sz2 = hidden_sz2\n",
    "        self.hidden_sz_lin1, self.hidden_sz_lin2, self.hidden_sz_lin3 = hidden_sz_lin1, hidden_sz_lin2, hidden_sz_lin3\n",
    "        self.lstm = nn.LSTM(input_size = len(keep_features),\n",
    "        hidden_size = hidden_sz1,\n",
    "        num_layers = lstm1_layers,\n",
    "        batch_first = True,\n",
    "        dropout = lstm1_dropout\n",
    "        )\n",
    "\n",
    "        self.milstm = MiLSTM(hidden_sz1, hidden_sz2)\n",
    "        self.self_attn_linear = nn.Linear(in_features=hidden_sz2, out_features=hidden_sz2, bias=True)\n",
    "        self.self_attn_v = nn.Parameter(torch.rand(hidden_sz2))\n",
    "\n",
    "        self.swdf_norm = nn.BatchNorm1d(nP+nN)\n",
    "        self.mainstream_norm = nn.BatchNorm1d(len(keep_features))\n",
    "        self.index_norm = nn.BatchNorm1d(1)\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_sz2, hidden_sz_lin1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_sz_lin1, hidden_sz_lin2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_sz_lin2, hidden_sz_lin3),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, swdf, mainstream, index, nP, nN):\n",
    "        \n",
    "        # Normalizing using nn.BatchNorm1d.\n",
    "        # swdf = self.swdf_norm(swdf.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        # mainstream = self.mainstream_norm(mainstream.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        # index = self.index_norm(index.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        \n",
    "        y_tilde, _ = self.lstm(mainstream)\n",
    "\n",
    "        Xps = []\n",
    "        for i_feature in range(nP):\n",
    "            Xpi, _ = self.lstm(swdf[:, :, i_feature].unsqueeze(2))\n",
    "            Xps.append(Xpi)\n",
    "        p_tilde = torch.stack(Xps).mean(axis=0)\n",
    "\n",
    "        Xns = []\n",
    "        for i_feature in range(nP, nP+nN):\n",
    "            Xni, _ = self.lstm(swdf[:, :,i_feature].unsqueeze(2))\n",
    "            Xns.append(Xni)\n",
    "        n_tilde = torch.stack(Xns).mean(axis=0)\n",
    "        \n",
    "        index_tilde, _ = self.lstm(index)\n",
    "        # print(y_tilde.shape, p_tilde.shape, n_tilde.shape, index_tilde.shape)\n",
    "        \n",
    "        y_tilde_prime, _ = self.milstm(y_tilde, p_tilde, n_tilde, index_tilde)\n",
    "\n",
    "        js_ = self.self_attn_linear(y_tilde_prime)\n",
    "        js = torch.tanh(js_).matmul(self.self_attn_v)\n",
    "        betas = torch.softmax(js, dim=1)\n",
    "        y_ = torch.matmul(betas.unsqueeze(1), y_tilde_prime).squeeze()\n",
    "        y_hat = self.regressor(y_).squeeze()\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    mse_record = []\n",
    "    mape_record = []\n",
    "    model.train()\n",
    "    for i, sample in enumerate(dataloader):\n",
    "        swdf, mainstream, index, y_act = sample\n",
    "        swdf, mainstream, index, y_act = swdf.float(), mainstream.float(), index.float(), y_act.float()\n",
    "        pred = model(swdf, mainstream, index, dataloader.dataset.nP, dataloader.dataset.nN)\n",
    "        loss = loss_fn(pred, y_act)\n",
    "        mape = torch.mean((torch.abs((y_act - pred) / y_act)) * 100)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        mse_record.append(loss)\n",
    "        mape_record.append(mape)\n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            loss, current = loss.item(), i * len(swdf)\n",
    "            print(f\"MSE loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return mse_record, mape_record\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    mse_record = []\n",
    "    mape_record = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(dataloader):\n",
    "            swdf, mainstream, index, y_act = sample\n",
    "            swdf, mainstream, index, y_act = swdf.float(), mainstream.float(), index.float(), y_act.float()\n",
    "            pred = model(swdf, mainstream, index, dataloader.dataset.nP, dataloader.dataset.nN)\n",
    "            loss = loss_fn(pred, y_act)\n",
    "            mape = torch.mean((torch.abs((y_act - pred) / y_act)) * 100)\n",
    "            mse_record.append(loss)\n",
    "            mape_record.append(mape)\n",
    "    \n",
    "    return mse_record, mape_record\n",
    "            \n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_test(train_dataset_path, test_datset_path, batch_size, keep_features, \n",
    "                  hidden_sz1, hidden_sz2, hidden_sz_lin1, hidden_sz_lin2, hidden_sz_lin3,\n",
    "                  learning_rate, epochs, clip_gradients=False):\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "    train_dataset = load_NSEDataset(train_dataset_path)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    nP, nN = train_dataloader.dataset.nP, train_dataloader.dataset.nN \n",
    "\n",
    "    model = NeuralNetwork(keep_features=keep_features, hidden_sz1=hidden_sz1, hidden_sz2=hidden_sz2, \n",
    "    hidden_sz_lin1=hidden_sz_lin1, hidden_sz_lin2=hidden_sz_lin2, hidden_sz_lin3=hidden_sz_lin3, nP=nP, nN=nN).to(device)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    if clip_gradients:\n",
    "        for p in model.parameters():\n",
    "            p.register_hook(lambda grad: torch.clamp(grad, -50, 50))\n",
    "\n",
    "    mse_record = []\n",
    "    mape_record = []\n",
    "    mse_epochs = []\n",
    "    mape_epochs = []\n",
    "    for i_epoch in range(epochs):\n",
    "        mse_, mape_ = train(train_dataloader, model, loss_fn, optimizer)\n",
    "        mse_record.extend(mse_)\n",
    "        mse_epochs.append(mse_[-1])\n",
    "        mape_record.extend(mape_)\n",
    "        mape_epochs.append(mape_[-1])\n",
    "        \n",
    "    print(\"Training completed for {}\".format(train_dataset_path))\n",
    "    \n",
    "    min_mse_all = min(mse_record)\n",
    "    avg_mse_all = torch.mean(torch.stack(mse_record))\n",
    "    min_mape_all = min(mape_record)\n",
    "    avg_mape_all = torch.mean(torch.stack(mape_record))\n",
    "    \n",
    "    test_dataset = load_NSEDataset(test_dataset_path)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    mse_test, mape_test = test_loop(test_dataloader, model, loss_fn)\n",
    "    print(\"Testing completed for {}\".format(test_dataset_path))\n",
    "    \n",
    "    with open('results.csv', 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow([datetime.datetime.now(), min_mse_all.item(), avg_mse_all.item(), min_mape_all.item(), avg_mape_all.item(), mse_test[0].item(), mape_test[0].item(), nP, nN, epochs, learning_rate, train_dataset.min_chosen_p, train_dataset.max_chosen_n, test_dataset.min_chosen_p, test_dataset.max_chosen_n, train_dataset_path, test_dataset_path])\n",
    "        \n",
    "    print(\"Results file updated.\")\n",
    "    return mse_test, mape_test\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2683: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "c:\\python3.9.2\\lib\\site-packages\\numpy\\lib\\function_base.py:2542: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created for ITC.NS\n",
      "Dataset created for ITC.NS\n",
      "Using cpu device\n",
      "MSE loss: 0.466839  [    0/ 1266]\n",
      "MSE loss: 0.202830  [  512/ 1266]\n",
      "MSE loss: 0.136539  [  484/ 1266]\n",
      "MSE loss: 0.444315  [    0/ 1266]\n",
      "MSE loss: 0.069062  [  512/ 1266]\n",
      "MSE loss: 2.595742  [  484/ 1266]\n",
      "MSE loss: 0.295783  [    0/ 1266]\n",
      "MSE loss: 0.095629  [  512/ 1266]\n",
      "MSE loss: 0.032952  [  484/ 1266]\n",
      "MSE loss: 0.229657  [    0/ 1266]\n",
      "MSE loss: 0.066126  [  512/ 1266]\n",
      "MSE loss: 0.011758  [  484/ 1266]\n",
      "MSE loss: 0.163213  [    0/ 1266]\n",
      "MSE loss: 0.046082  [  512/ 1266]\n",
      "MSE loss: 0.003533  [  484/ 1266]\n",
      "MSE loss: 0.110856  [    0/ 1266]\n",
      "MSE loss: 0.041381  [  512/ 1266]\n",
      "MSE loss: 0.007456  [  484/ 1266]\n",
      "MSE loss: 0.081828  [    0/ 1266]\n",
      "MSE loss: 0.045297  [  512/ 1266]\n",
      "MSE loss: 0.012897  [  484/ 1266]\n",
      "MSE loss: 0.070082  [    0/ 1266]\n",
      "MSE loss: 0.048075  [  512/ 1266]\n",
      "MSE loss: 0.015582  [  484/ 1266]\n",
      "MSE loss: 0.065470  [    0/ 1266]\n",
      "MSE loss: 0.049181  [  512/ 1266]\n",
      "MSE loss: 0.016780  [  484/ 1266]\n",
      "MSE loss: 0.063330  [    0/ 1266]\n",
      "MSE loss: 0.049597  [  512/ 1266]\n",
      "MSE loss: 0.017342  [  484/ 1266]\n",
      "MSE loss: 0.062230  [    0/ 1266]\n",
      "MSE loss: 0.049755  [  512/ 1266]\n",
      "MSE loss: 0.017636  [  484/ 1266]\n",
      "MSE loss: 0.061595  [    0/ 1266]\n",
      "MSE loss: 0.049814  [  512/ 1266]\n",
      "MSE loss: 0.017806  [  484/ 1266]\n",
      "MSE loss: 0.061200  [    0/ 1266]\n",
      "MSE loss: 0.049833  [  512/ 1266]\n",
      "MSE loss: 0.017911  [  484/ 1266]\n",
      "MSE loss: 0.060941  [    0/ 1266]\n",
      "MSE loss: 0.049835  [  512/ 1266]\n",
      "MSE loss: 0.017980  [  484/ 1266]\n",
      "MSE loss: 0.060766  [    0/ 1266]\n",
      "MSE loss: 0.049829  [  512/ 1266]\n",
      "MSE loss: 0.018028  [  484/ 1266]\n",
      "MSE loss: 0.060645  [    0/ 1266]\n",
      "MSE loss: 0.049820  [  512/ 1266]\n",
      "MSE loss: 0.018058  [  484/ 1266]\n",
      "MSE loss: 0.060571  [    0/ 1266]\n",
      "MSE loss: 0.049807  [  512/ 1266]\n",
      "MSE loss: 0.018082  [  484/ 1266]\n",
      "MSE loss: 0.060520  [    0/ 1266]\n",
      "MSE loss: 0.049796  [  512/ 1266]\n",
      "MSE loss: 0.018094  [  484/ 1266]\n",
      "MSE loss: 0.060496  [    0/ 1266]\n",
      "MSE loss: 0.049782  [  512/ 1266]\n",
      "MSE loss: 0.018103  [  484/ 1266]\n",
      "MSE loss: 0.060487  [    0/ 1266]\n",
      "MSE loss: 0.049769  [  512/ 1266]\n",
      "MSE loss: 0.018106  [  484/ 1266]\n",
      "Training completed for data_collection/pickled_datasets/min-max_Normalized_ITC_2017-01_full_w10_t20_p10_n10_o.pkl\n",
      "Testing completed for data_collection/pickled_datasets/min-max_Normalized_ITC_2021-05_full_w10_t20_p10_n10_o.pkl\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'results.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-88b74d36e18c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     train_and_test(train_dataset_path, test_dataset_path, batch_size, keep_features, \n\u001b[0m\u001b[0;32m     67\u001b[0m                       \u001b[0mhidden_sz1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_sz2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_sz_lin1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_sz_lin2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_sz_lin3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                       learning_rate, epochs, clip_gradients=False);\n",
      "\u001b[1;32m<ipython-input-15-a48b69e5e51c>\u001b[0m in \u001b[0;36mtrain_and_test\u001b[1;34m(train_dataset_path, test_datset_path, batch_size, keep_features, hidden_sz1, hidden_sz2, hidden_sz_lin1, hidden_sz_lin2, hidden_sz_lin3, learning_rate, epochs, clip_gradients)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Testing completed for {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'results.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_mse_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_mse_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_mape_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_mape_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmse_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmape_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_chosen_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_chosen_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_chosen_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_chosen_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset_path\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'results.csv'"
     ]
    }
   ],
   "source": [
    "for nP in [2, 4, 8, 10, 15, 20, 50, 100, 120, 166]:\n",
    "    nN = nP\n",
    "    ohlcv_dir = 'data_collection/ohlcv_data'\n",
    "    target_ticker = 'ITC.NS'\n",
    "    target_ticker_file = 'ohlcv_fmcg.csv'\n",
    "    len_window = 10\n",
    "    len_corr_traceback = 20\n",
    "    nP = 10\n",
    "    nN = 10\n",
    "    keep_feat = 'o'\n",
    "    train_start_date = '2017-01-01'\n",
    "    train_end_date = None\n",
    "    # For predictions to start from day i (2021-06-01), \n",
    "    # i should be (len_corr_traceback)th day in the test_dataloader.\n",
    "    test_start_date = '2021-05-13'\n",
    "    test_end_date = None\n",
    "    normalize = 'min-max'\n",
    "    # normalize = None\n",
    "\n",
    "    train_dataset = NSEDataset(ohlcv_dir=ohlcv_dir, \n",
    "                         target_ticker=target_ticker, \n",
    "                         target_ticker_file=target_ticker_file, \n",
    "                         len_window=len_window,\n",
    "                         len_corr_traceback=len_corr_traceback, \n",
    "                         nP=nP,\n",
    "                         nN=nN, \n",
    "                         keep_feat=keep_feat, \n",
    "                         start_date=train_start_date,\n",
    "                         end_date=train_end_date,\n",
    "                         normalize=normalize)\n",
    "\n",
    "    test_dataset = NSEDataset(ohlcv_dir=ohlcv_dir, \n",
    "                         target_ticker=target_ticker, \n",
    "                         target_ticker_file=target_ticker_file, \n",
    "                         len_window=len_window,\n",
    "                         len_corr_traceback=len_corr_traceback, \n",
    "                         nP=nP,\n",
    "                         nN=nN, \n",
    "                         keep_feat=keep_feat, \n",
    "                         start_date=test_start_date,\n",
    "                         end_date=test_end_date,\n",
    "                         normalize=normalize)\n",
    "\n",
    "    train_end_date_str = train_end_date[:7] if train_end_date is not None else 'full'\n",
    "    test_end_date_str = test_end_date[:7] if test_end_date is not None else 'full'\n",
    "\n",
    "    train_dataset_path = 'data_collection/pickled_datasets/{}_Normalized_{}_{}_{}_w{}_t{}_p{}_n{}_{}.pkl'.format(\n",
    "    normalize, target_ticker[:-3], train_start_date[:7], train_end_date_str, len_window, len_corr_traceback, nP, nN, keep_feat)\n",
    "\n",
    "    test_dataset_path = 'data_collection/pickled_datasets/{}_Normalized_{}_{}_{}_w{}_t{}_p{}_n{}_{}.pkl'.format(\n",
    "    normalize, target_ticker[:-3], test_start_date[:7], test_end_date_str, len_window, len_corr_traceback, nP, nN, keep_feat)\n",
    "\n",
    "    save_NSEDataset(train_dataset, train_dataset_path)\n",
    "    save_NSEDataset(test_dataset, test_dataset_path)\n",
    "\n",
    "    batch_size = 512\n",
    "    keep_features = 'o'\n",
    "    hidden_sz1 = 64\n",
    "    hidden_sz2 = 64\n",
    "    hidden_sz_lin1 = 64\n",
    "    hidden_sz_lin2 = 32\n",
    "    hidden_sz_lin3 = 1\n",
    "    learning_rate = 0.001\n",
    "    epochs = 20\n",
    "\n",
    "    train_and_test(train_dataset_path, test_dataset_path, batch_size, keep_features, \n",
    "                      hidden_sz1, hidden_sz2, hidden_sz_lin1, hidden_sz_lin2, hidden_sz_lin3,\n",
    "                      learning_rate, epochs, clip_gradients=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 1\n",
    "# mse_test, mape_test = train_and_test(train_dataset_path, test_dataset_path, batch_size, keep_features, \n",
    "#                   hidden_sz1, hidden_sz2, hidden_sz_lin1, hidden_sz_lin2, hidden_sz_lin3,\n",
    "#                   learning_rate, epochs, clip_gradients=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
