{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view as sliding_window_view\n",
    "import pickle\n",
    "import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSEDataset(Dataset):\n",
    "    def __init__(self, ohlcv_dir, target_ticker, target_ticker_file, len_window, len_corr_traceback, nP, nN, \n",
    "    keep_tickers=None, ohlcv_prefix='', ohlcv_sufix='', ohlcv_files=None, start_date=None, end_date=None,\n",
    "    target_feat='c', keep_feat='ohlcva', normalize='min-max'):\n",
    "\n",
    "        feat_name_map = {\n",
    "            'o' : 'Open', \n",
    "            'h' : 'High', \n",
    "            'l' : 'Low', \n",
    "            'c' : 'Close', \n",
    "            'v' : 'Volume',\n",
    "            'a' : 'Adj Close'\n",
    "        }\n",
    "\n",
    "        self.len_window = len_window\n",
    "        self.len_corr_traceback = len_corr_traceback\n",
    "        self.nP, self.nN = nP, nN\n",
    "        self.target_feat = target_feat\n",
    "        self.keep_feat = keep_feat\n",
    "        self.start_date, self.end_date = start_date, end_date\n",
    "\n",
    "        if ohlcv_files is not None:\n",
    "            ohlcv_files = set(ohlcv_files)\n",
    "        \n",
    "        if keep_tickers is not None:\n",
    "            keep_tickers = set(keep_tickers)\n",
    "    \n",
    "        df = pd.read_csv(os.path.join(ohlcv_dir, target_ticker_file))\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "        if start_date is not None:\n",
    "            start_mask =  df['Date'] >= datetime.datetime.fromisoformat(start_date)\n",
    "            i_start = start_mask[start_mask].index.min()\n",
    "        else:\n",
    "            i_start = 0\n",
    "        \n",
    "        if end_date is not None:\n",
    "            end_mask =  df['Date'] > datetime.datetime.fromisoformat(end_date)\n",
    "            i_end = end_mask[end_mask].index.min()\n",
    "        else:\n",
    "            i_end = len(df)\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.set_index(['Date', 'Ticker'], inplace=True)\n",
    "        df = df.iloc[i_start:i_end]\n",
    "        \n",
    "        self.mainstream_df = df.loc[:, target_ticker]\n",
    "        self.df = df.drop(target_ticker, axis=1)\n",
    "        \n",
    "        if ohlcv_files is not None and target_ticker_file not in ohlcv_files:\n",
    "            self.df = pd.DataFrame(columns=df.columns)\n",
    "            \n",
    "        for f in os.listdir(ohlcv_dir):\n",
    "            if f.startswith(ohlcv_prefix) and f.endswith(ohlcv_sufix) and (ohlcv_files is None or f in ohlcv_files):\n",
    "                if f == target_ticker_file:\n",
    "                    continue\n",
    "                else:\n",
    "                    temp_df = pd.read_csv(os.path.join(ohlcv_dir, f))\n",
    "                    temp_df.reset_index(drop=True, inplace=True)\n",
    "                    temp_df['Date'] = pd.to_datetime(temp_df['Date'])\n",
    "                    self.df = pd.merge(self.df.reset_index(), temp_df, on=['Date', 'Ticker'],\n",
    "                    how='inner', suffixes=('', '_y')).set_index(['Date', 'Ticker'])\n",
    "                    self.df.drop(self.df.filter(regex='_y$').columns.tolist(), axis=1, inplace=True)\n",
    "                    \n",
    "        if keep_tickers is not None:\n",
    "            for c in self.df:\n",
    "                if c not in keep_tickers:\n",
    "                    self.df.drop(c, axis=1, inplace=True)\n",
    "\n",
    "        \n",
    "        self.df = self.df.pivot_table(index='Date', columns='Ticker')\n",
    "        self.df.columns = self.df.columns.map('_'.join)\n",
    "        if isinstance(self.mainstream_df, pd.Series):\n",
    "            self.mainstream_df = pd.DataFrame(self.mainstream_df)\n",
    "        \n",
    "        self.mainstream_df = self.mainstream_df.pivot_table(index='Date', columns='Ticker')\n",
    "        self.mainstream_df.columns = self.mainstream_df.columns.map('_'.join)\n",
    "\n",
    "        target_feat_name = \"{}_{}\".format(target_ticker, feat_name_map[target_feat])\n",
    "        \n",
    "        self.unshifted_target = self.mainstream_df.loc[:, target_feat_name]\n",
    "        self.target = self.unshifted_target.shift(periods=-1).iloc[:-1]\n",
    "\n",
    "        # Min-max normalizing.\n",
    "        if normalize == 'min-max':\n",
    "            self.target = (self.target - self.target.min())/(self.target.max() - self.target.min())\n",
    "        \n",
    "        # To account for absence of target for last row.\n",
    "        self.df = self.df.iloc[:-1, :]  \n",
    "        self.mainstream_df = self.mainstream_df.iloc[:-1, :]\n",
    "        \n",
    "        # Min-max Normalizing\n",
    "        if normalize == 'min-max':\n",
    "            self.df = (self.df-self.df.min())/(self.df.max()-self.df.min())\n",
    "            self.mainstream_df = (self.mainstream_df-self.mainstream_df.min())/(self.mainstream_df.max()-self.mainstream_df.min())\n",
    "        \n",
    "        drop_features = set(feat_name_map.keys()).difference({feat for feat in keep_feat})\n",
    "        for feat in drop_features:\n",
    "            self.df.drop(self.df.filter(regex='_{}$'.format(feat_name_map[feat])).columns.tolist(), axis=1, inplace=True)\n",
    "            self.mainstream_df.drop(self.mainstream_df.filter(regex='_{}$'.format(feat_name_map[feat])).columns.tolist(), axis=1, inplace=True)\n",
    "\n",
    "        # For i_end, data of [i_end - (len_corr_traceback) : i_end] (py notation)\n",
    "        # is needed to calculate correlation, basis on which data of \n",
    "        # [i_end - (len_window) : i_end] (py notation) must be sent.\n",
    "        # i_end is excluded, so last i_end should be len(self.df)\n",
    "        \n",
    "        self.swdf = []\n",
    "        for i_end in range(len_corr_traceback, len(self.df)+1):\n",
    "            if i_end % 50 == 0:\n",
    "                print(i_end, len(self.df))\n",
    "            self.swdf.append(self.get_high_corr(self.unshifted_target.iloc[i_end-len_corr_traceback:i_end], \n",
    "            self.df.iloc[i_end-len_corr_traceback:i_end, :], len_window, nP, nN))\n",
    "            \n",
    "        # self.swdf = np.array(self.swdf).reshape(len(self.swdf), self.len_window, -1)\n",
    "        self.swdf = np.array(self.swdf)\n",
    "        self.swdf = pd.DataFrame(self.swdf).fillna(method='ffill').to_numpy()\n",
    "        \n",
    "        # if earlier self.df.shape was (6(n+1), c), it should now be\n",
    "        # (n, c), mainstream_df.shape and index_data_df should be (n, 1) and swdf.shape\n",
    "        # should be (n-lct+1, lw*(nP+nN)).\n",
    "\n",
    "        # For index 0, \n",
    "        # swdf[0], mainstream_df[lct-lw : lct] flattend (py notation)\n",
    "        # index_data_df[lct-lw : lct] flattened (py notation) should be accessed.\n",
    "\n",
    "        # For index i < len(swdf), \n",
    "        # swdf[i], mainstream_df[lct-lw + i: lct + i] flattend (py notation)\n",
    "        # index_data_df[lct-lw + i : lct + i] flattened (py notation) should be accessed.\n",
    "        # Correct. Continue from here. \n",
    "\n",
    "        # The index data used is for a single index.\n",
    "        self.index_data_df = pd.read_csv(\"data_collection/NIFTY 50.csv\")\n",
    "        self.index_data_df['Date'] = pd.to_datetime(self.index_data_df['Date'])\n",
    "        self.index_data_df.rename(columns={'SharesTraded' : 'Volume'}, inplace=True)\n",
    "        self.index_data_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "        if start_date is not None:\n",
    "            start_mask =  self.index_data_df['Date'] >= datetime.datetime.fromisoformat(start_date)\n",
    "            i_start = start_mask[start_mask].index.min()\n",
    "        else:\n",
    "            i_start = 0\n",
    "\n",
    "        if end_date is not None:\n",
    "            end_mask =  self.index_data_df['Date'] > datetime.datetime.fromisoformat(end_date)\n",
    "            i_end = end_mask[end_mask].index.min()\n",
    "        else:\n",
    "            i_end = len(self.index_data_df)\n",
    "\n",
    "        self.index_data_df.reset_index(drop=True, inplace=True)\n",
    "        self.index_data_df.set_index(['Date'], inplace=True)\n",
    "        self.index_data_df = self.index_data_df.iloc[i_start:i_end]\n",
    "\n",
    "        # Min-max normalizing.\n",
    "        if normalize == 'min-max':\n",
    "            self.index_data_df=(self.index_data_df-self.index_data_df.min())/(self.index_data_df.max()-self.index_data_df.min())\n",
    "\n",
    "        self.index_data_df = pd.DataFrame(self.index_data_df.loc[:, 'Close'])\n",
    "        self.index_data_df = self.index_data_df.reset_index().merge(\n",
    "    self.df.reset_index()['Date'], how='inner', on='Date').set_index('Date')\n",
    "\n",
    "        print(df.max(), self.df.max(), self.index_data_df.max())\n",
    "        \n",
    "\n",
    "    def get_high_corr(self, target: pd.Series, candidates: pd.DataFrame, len_window, nP, nN):\n",
    "        corr = candidates.corrwith(target)\n",
    "        p_best = corr.nlargest(nP)\n",
    "        n_best = corr.nsmallest(nN)\n",
    "        newrow = candidates.iloc[-len_window:, candidates.columns.get_indexer(p_best.index)].melt()['value'].tolist()\n",
    "        newrow.extend(candidates.iloc[-len_window:, candidates.columns.get_indexer(n_best.index)].melt()['value'].tolist())\n",
    "        return newrow\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.swdf)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For index i < len(swdf), \n",
    "        # swdf[i], mainstream_df[lct-lw + i: lct + i] flattend (py notation)\n",
    "        # index_data_df[lct-lw + i : lct + i] flattened (py notation) should be accessed.\n",
    "        \n",
    "        return (self.swdf[idx, :].reshape(dataset.len_window, -1), \n",
    "        self.mainstream_df.iloc[self.len_corr_traceback-self.len_window+idx : self.len_corr_traceback+idx].to_numpy(), \n",
    "        self.index_data_df.iloc[self.len_corr_traceback-self.len_window+idx : self.len_corr_traceback+idx].to_numpy(),\n",
    "        self.target[self.len_corr_traceback+idx-1]\n",
    "        )\n",
    "\n",
    "\n",
    "def save_NSEDataset(dataset, opfile):\n",
    "    with open(opfile, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "def load_NSEDataset(ipfile):\n",
    "    PICKLE_PROTOCOL = 4\n",
    "    with open(ipfile, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    " class MiLSTM(nn.Module):\n",
    "    def __init__(self, input_sz: int, hidden_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        self.p_size = input_sz \n",
    "        self.n_size = input_sz \n",
    "        self.index_size = input_sz\n",
    "        p_sz, n_sz, index_sz = self.p_size, self.n_size, self.index_size\n",
    "\n",
    "        #f_t\n",
    "        self.Wfh = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wfy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bf = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #o_t\n",
    "        self.Woh = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Woy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bo = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #c_t\n",
    "        self.Wch = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wcy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bc = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #c_pt\n",
    "        self.Wcph = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wcpp = nn.Parameter(torch.Tensor(p_sz, hidden_sz))\n",
    "        self.bcp = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        #c_nt\n",
    "        self.Wcnh = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wcnn = nn.Parameter(torch.Tensor(n_sz, hidden_sz))\n",
    "        self.bcn = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #c_it\n",
    "        self.Wcih = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wcii = nn.Parameter(torch.Tensor(index_sz, hidden_sz))\n",
    "        self.bci = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        #i_t\n",
    "        self.Wih = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wiy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bi = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #i_pt\n",
    "        self.Wiph = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wipy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bip = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        #c_nt\n",
    "        self.Winh = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Winy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bin = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #c_it\n",
    "        self.Wiih = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wiiy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bii = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        #attn\n",
    "        self.alpha_t = nn.Parameter(torch.Tensor(1))\n",
    "        self.alpha_pt = nn.Parameter(torch.Tensor(1))\n",
    "        self.alpha_nt = nn.Parameter(torch.Tensor(1))\n",
    "        self.alpha_it = nn.Parameter(torch.Tensor(1))\n",
    "        self.Wattn = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.ba = nn.Parameter(torch.Tensor(1))\n",
    "        self.bap = nn.Parameter(torch.Tensor(1))\n",
    "        self.ban = nn.Parameter(torch.Tensor(1))\n",
    "        self.bai = nn.Parameter(torch.Tensor(1))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        #c_pt\n",
    "        nn.init.zeros_(self.Wcph)\n",
    "        nn.init.zeros_(self.Wcpp)\n",
    "        nn.init.zeros_(self.bcp)\n",
    "        \n",
    "        #c_nt\n",
    "        nn.init.zeros_(self.Wcnh)\n",
    "        nn.init.zeros_(self.Wcnn)\n",
    "        nn.init.zeros_(self.bcn)\n",
    "        \n",
    "        #c_it\n",
    "        nn.init.zeros_(self.Wcih)\n",
    "        nn.init.zeros_(self.Wcii)\n",
    "        nn.init.zeros_(self.bci)\n",
    "        \n",
    "\n",
    "    def forward(self, y_tilde, p_tilde, n_tilde, index_tilde, init_stats=None):\n",
    "        batch_size, win_len, _ = y_tilde.shape\n",
    "        hidden_seqs = []\n",
    "        cell_states = []\n",
    "\n",
    "        if init_stats is None:\n",
    "            h_t, cell_t = (torch.zeros(batch_size, self.hidden_size).to(y_tilde.device), \n",
    "                        torch.zeros(batch_size, self.hidden_size).to(y_tilde.device))\n",
    "        else:\n",
    "            h_t, cell_t = init_states \n",
    "\n",
    "        \n",
    "        for t in range(win_len):\n",
    "            y_t = y_tilde[:, t, :]\n",
    "            p_t = p_tilde[:, t, :]\n",
    "            n_t = n_tilde[:, t, :]\n",
    "            index_t = index_tilde[:, t, :]\n",
    "\n",
    "            f_t = torch.sigmoid(y_t @ self.Wfy + h_t @ self.Wfh + self.bf)\n",
    "            o_t = torch.sigmoid(y_t @ self.Woy + h_t @ self.Woh + self.bo)\n",
    "            c_t = torch.tanh(y_t @ self.Wcy + h_t @ self.Wch + self.bc)\n",
    "            c_pt = torch.tanh(p_t @ self.Wcpp + h_t @ self.Wcph + self.bcp)\n",
    "            c_nt = torch.tanh(n_t @ self.Wcnn + h_t @ self.Wcnh + self.bcn)\n",
    "            c_it = torch.tanh(index_t @ self.Wcii + h_t @ self.Wcph + self.bci)\n",
    "\n",
    "            i_t = torch.sigmoid(y_t @ self.Wiy + h_t @ self.Wih + self.bi)\n",
    "            i_pt = torch.sigmoid(y_t @ self.Wipy + h_t @ self.Wiph + self.bip)\n",
    "            i_nt = torch.sigmoid(y_t @ self.Winy + h_t @ self.Winh + self.bin)\n",
    "            i_it  = torch.sigmoid(y_t @ self.Wiiy + h_t @ self.Wiih + self.bii)\n",
    "\n",
    "            l_t = torch.mul(c_t, i_t)\n",
    "            l_pt = torch.mul(c_pt, i_pt)\n",
    "            l_nt = torch.mul(c_nt, i_nt)\n",
    "            l_it = torch.mul(c_it, i_it)\n",
    "           \n",
    "            u_t = torch.mul(l_t @ self.Wattn, cell_t).sum(dim=1)\n",
    "            u_pt = torch.mul(l_pt @ self.Wattn, cell_t).sum(dim=1)\n",
    "            u_nt = torch.mul(l_nt @ self.Wattn, cell_t).sum(dim=1)\n",
    "            u_it = torch.mul(l_it @ self.Wattn, cell_t).sum(dim=1)\n",
    "\n",
    "            alphas = torch.stack((u_t, u_pt, u_nt, u_it), dim=1)\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            probs = softmax(alphas)\n",
    "            alpha_t, alpha_pt, alpha_nt, alpha_it = probs[:, 0], probs[:, 1], probs[:, 2], probs[:, 3]\n",
    "            \n",
    "            L_t = self.alpha_t*l_t + self.alpha_pt*l_pt + self.alpha_nt*l_nt + self.alpha_it*l_it\n",
    "\n",
    "            cell_t = torch.mul(cell_t, f_t) + L_t\n",
    "            h_t = torch.mul(torch.tanh(cell_t), o_t)\n",
    "            \n",
    "            hidden_seqs.append(h_t)\n",
    "            cell_states.append(cell_t)\n",
    "\n",
    "        hidden_seqs = torch.stack(hidden_seqs)\n",
    "        hidden_seqs = hidden_seqs.transpose(0, 1).contiguous()\n",
    "        return hidden_seqs, (h_t, cell_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, keep_features, hidden_sz1, hidden_sz2, hidden_sz_lin1, hidden_sz_lin2, hidden_sz_lin3, nP, nN, lstm1_layers=1, lstm1_dropout=0):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden_sz1 = hidden_sz1\n",
    "        self.hidden_sz2 = hidden_sz2\n",
    "        self.hidden_sz_lin1, self.hidden_sz_lin2, self.hidden_sz_lin3 = hidden_sz_lin1, hidden_sz_lin2, hidden_sz_lin3\n",
    "        self.lstm = nn.LSTM(input_size = len(keep_features),\n",
    "        hidden_size = hidden_sz1,\n",
    "        num_layers = lstm1_layers,\n",
    "        batch_first = True,\n",
    "        dropout = lstm1_dropout\n",
    "        )\n",
    "\n",
    "        self.milstm = MiLSTM(hidden_sz1, hidden_sz2)\n",
    "        self.self_attn_linear = nn.Linear(in_features=hidden_sz2, out_features=hidden_sz2, bias=True)\n",
    "        self.self_attn_v = nn.Parameter(torch.rand(hidden_sz2))\n",
    "\n",
    "        self.swdf_norm = nn.BatchNorm1d(nP+nN)\n",
    "        self.mainstream_norm = nn.BatchNorm1d(len(keep_features))\n",
    "        self.index_norm = nn.BatchNorm1d(1)\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_sz2, hidden_sz_lin1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_sz_lin1, hidden_sz_lin2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_sz_lin2, hidden_sz_lin3),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, swdf, mainstream, index, nP, nN):\n",
    "        \n",
    "        # Normalizing using nn.BatchNorm1d.\n",
    "        # swdf = self.swdf_norm(swdf.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        # mainstream = self.mainstream_norm(mainstream.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        # index = self.index_norm(index.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        \n",
    "        y_tilde, _ = self.lstm(mainstream)\n",
    "\n",
    "        Xps = []\n",
    "        for i_feature in range(nP):\n",
    "            Xpi, _ = self.lstm(swdf[:, :, i_feature].unsqueeze(2))\n",
    "            Xps.append(Xpi)\n",
    "        p_tilde = torch.stack(Xps).mean(axis=0)\n",
    "\n",
    "        Xns = []\n",
    "        for i_feature in range(nP, nP+nN):\n",
    "            Xni, _ = self.lstm(swdf[:, :,i_feature].unsqueeze(2))\n",
    "            Xns.append(Xni)\n",
    "        n_tilde = torch.stack(Xns).mean(axis=0)\n",
    "        \n",
    "        index_tilde, _ = self.lstm(index)\n",
    "        # print(y_tilde.shape, p_tilde.shape, n_tilde.shape, index_tilde.shape)\n",
    "        \n",
    "        y_tilde_prime, _ = self.milstm(y_tilde, p_tilde, n_tilde, index_tilde)\n",
    "\n",
    "        js_ = self.self_attn_linear(y_tilde_prime)\n",
    "        js = torch.tanh(js_).matmul(self.self_attn_v)\n",
    "        betas = torch.softmax(js, dim=1)\n",
    "        y_ = torch.matmul(betas.unsqueeze(1), y_tilde_prime).squeeze()\n",
    "        y_hat = self.regressor(y_).squeeze()\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    mse_record = []\n",
    "    mape_record = []\n",
    "    model.train()\n",
    "    for i, sample in enumerate(dataloader):\n",
    "        swdf, mainstream, index, y_act = sample\n",
    "        swdf, mainstream, index, y_act = swdf.float(), mainstream.float(), index.float(), y_act.float()\n",
    "        pred = model(swdf, mainstream, index, dataloader.dataset.nP, dataloader.dataset.nN)\n",
    "        loss = loss_fn(pred, y_act)\n",
    "        mape = np.mean((np.abs((y_act - pred) / y_act)) * 100)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        mse_record.append(loss)\n",
    "        mape_record.append(mape)\n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            loss, current = loss.item(), i * len(swdf)\n",
    "            print(f\"MSE loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return mse_record, mape_record\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 1285\n",
      "100 1285\n",
      "150 1285\n",
      "200 1285\n",
      "250 1285\n",
      "300 1285\n",
      "350 1285\n",
      "400 1285\n",
      "450 1285\n",
      "500 1285\n",
      "550 1285\n",
      "600 1285\n",
      "650 1285\n",
      "700 1285\n",
      "750 1285\n",
      "800 1285\n",
      "850 1285\n",
      "900 1285\n",
      "950 1285\n",
      "1000 1285\n",
      "1050 1285\n",
      "1100 1285\n",
      "1150 1285\n",
      "1200 1285\n",
      "1250 1285\n",
      "BRITANNIA.NS       6269128.0\n",
      "COLPAL.NS          7265730.0\n",
      "DABUR.NS          18573319.0\n",
      "EMAMILTD.NS       22507454.0\n",
      "GODREJCP.NS       27379767.0\n",
      "HINDUNILVR.NS    185669922.0\n",
      "ITC.NS           149479681.0\n",
      "MARICO.NS         21636785.0\n",
      "MCDOWELL-N.NS     30119580.0\n",
      "NESTLEIND.NS       1106746.0\n",
      "PGHH.NS             131452.0\n",
      "RADICO.NS         10428618.0\n",
      "TATACONSUM.NS     63551245.0\n",
      "UBL.NS             5523161.0\n",
      "VBL.NS             9872874.0\n",
      "dtype: float64 ABBOTINDIA.NS_Open    1.0\n",
      "ADANIENT.NS_Open      1.0\n",
      "ALKEM.NS_Open         1.0\n",
      "AMARAJABAT.NS_Open    1.0\n",
      "AMBER.NS_Open         1.0\n",
      "                     ... \n",
      "VOLTAS.NS_Open        1.0\n",
      "WELCORP.NS_Open       1.0\n",
      "WHIRLPOOL.NS_Open     1.0\n",
      "WIPRO.NS_Open         1.0\n",
      "ZEEL.NS_Open          1.0\n",
      "Length: 166, dtype: float64 Close    1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "ohlcv_dir = 'data_collection/ohlcv_data'\n",
    "target_ticker = 'ITC.NS'\n",
    "target_ticker_file = 'ohlcv_fmcg.csv'\n",
    "len_window = 10\n",
    "len_corr_traceback = 20\n",
    "nP = 10\n",
    "nN = 10\n",
    "keep_feat = 'o'\n",
    "train_start_date = '2016-01-01'\n",
    "train_end_date = '2021-05-31'\n",
    "# For predictions to start from day i (2021-06-01), \n",
    "# i should be (len_corr_traceback)th day in the test_dataloader.\n",
    "test_start_date = '2021-05-13'\n",
    "test_end_date = None\n",
    "normalize = 'min-max'\n",
    "                        \n",
    "train_dataset = NSEDataset(ohlcv_dir=ohlcv_dir, \n",
    "                     target_ticker=target_ticker, \n",
    "                     target_ticker_file=target_ticker_file, \n",
    "                     len_window=len_window\n",
    "                     len_corr_traceback=len_corr_traceback, \n",
    "                     nP=nP\n",
    "                     nN=nN, \n",
    "                     keep_feat=keep_feat, \n",
    "                     start_date=train_start_date,\n",
    "                     end_date=train_end_date,\n",
    "                     normalize=normalize)\n",
    "\n",
    "test_dataset = NSEDataset(ohlcv_dir=ohlcv_dir, \n",
    "                     target_ticker=target_ticker, \n",
    "                     target_ticker_file=target_ticker_file, \n",
    "                     len_window=len_window\n",
    "                     len_corr_traceback=len_corr_traceback, \n",
    "                     nP=nP\n",
    "                     nN=nN, \n",
    "                     keep_feat=keep_feat, \n",
    "                     start_date=test_start_date,\n",
    "                     end_date=test_end_date,\n",
    "                     normalize=normalize)\n",
    "\n",
    "save_NSEDataset(train_dataset, 'data_collection/pickled_datasets/{}_Normalized_{}_{}_{}_w{}_t{}_p{}_n{}_{}.pkl'.format(\n",
    "normalize, target_ticker[:-3], train_start_date[:7], train_end_date[:7], len_window, len_corr_traceback, nP, nN, keep_feat))\n",
    "\n",
    "save_NSEDataset(test_dataset, 'data_collection/pickled_datasets/{}_Normalized_{}_{}_{}_w{}_t{}_p{}_n{}_{}.pkl'.format(\n",
    "normalize, target_ticker[:-3], test_start_date[:7], test_end_date[:7], len_window, len_corr_traceback, nP, nN, keep_feat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Epoch 1 -----\n",
      "\n",
      "MSE loss: 0.465251  [    0/ 1266]\n",
      "MSE loss: 0.202197  [  512/ 1266]\n",
      "MSE loss: 0.127851  [  484/ 1266]\n",
      "Epoch 2 -----\n",
      "\n",
      "MSE loss: 0.319914  [    0/ 1266]\n",
      "MSE loss: 0.054351  [  512/ 1266]\n",
      "MSE loss: 0.038032  [  484/ 1266]\n",
      "Epoch 3 -----\n",
      "\n",
      "MSE loss: 0.212582  [    0/ 1266]\n",
      "MSE loss: 0.045004  [  512/ 1266]\n",
      "MSE loss: 0.005200  [  484/ 1266]\n",
      "Epoch 4 -----\n",
      "\n",
      "MSE loss: 0.108001  [    0/ 1266]\n",
      "MSE loss: 0.088233  [  512/ 1266]\n",
      "MSE loss: 0.004018  [  484/ 1266]\n",
      "Epoch 5 -----\n",
      "\n",
      "MSE loss: 0.115569  [    0/ 1266]\n",
      "MSE loss: 0.047972  [  512/ 1266]\n",
      "MSE loss: 0.009521  [  484/ 1266]\n",
      "Epoch 6 -----\n",
      "\n",
      "MSE loss: 0.086438  [    0/ 1266]\n",
      "MSE loss: 0.056447  [  512/ 1266]\n",
      "MSE loss: 0.012257  [  484/ 1266]\n",
      "Epoch 7 -----\n",
      "\n",
      "MSE loss: 0.080826  [    0/ 1266]\n",
      "MSE loss: 0.056352  [  512/ 1266]\n",
      "MSE loss: 0.013245  [  484/ 1266]\n",
      "Epoch 8 -----\n",
      "\n",
      "MSE loss: 0.077509  [    0/ 1266]\n",
      "MSE loss: 0.055816  [  512/ 1266]\n",
      "MSE loss: 0.013891  [  484/ 1266]\n",
      "Epoch 9 -----\n",
      "\n",
      "MSE loss: 0.074920  [    0/ 1266]\n",
      "MSE loss: 0.055411  [  512/ 1266]\n",
      "MSE loss: 0.014358  [  484/ 1266]\n",
      "Epoch 10 -----\n",
      "\n",
      "MSE loss: 0.072813  [    0/ 1266]\n",
      "MSE loss: 0.055104  [  512/ 1266]\n",
      "MSE loss: 0.014710  [  484/ 1266]\n",
      "Min. MSE : 0.004018190782517195 \n",
      "Avg. MSE : 0.08612645417451859 \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def train_and_test(train_dataset_path, test_datset_path, batch_size, keep_features, \n",
    "                  hidden_sz1, hidden_sz2, hidden_sz_lin1, hidden_sz_lin2, hidden_sz_lin3,\n",
    "                  learning_rate, epochs, clip_gradients=False):\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "    train_dataset = load_NSEDataset(train_dataset_path)\n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    nP, nN = train_dataloader.dataset.nP, train_dataloader.dataset.nN \n",
    "\n",
    "    model = NeuralNetwork(keep_features=keep_features, hidden_sz1=hidden_sz1, hidden_sz2=hidden_sz2, \n",
    "    hidden_sz_lin1=hidden_sz_lin1, hidden_sz_lin2=hidden_sz_lin2, hidden_sz_lin3=hidden_sz_lin3, nP=nP, nN=nN).to(device)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    if clip_gradients:\n",
    "        for p in model.parameters():\n",
    "            p.register_hook(lambda grad: torch.clamp(grad, -50, 50))\n",
    "\n",
    "    mse_record = []\n",
    "    mape_record = []\n",
    "    mse_epochs = []\n",
    "    mape_epochs = []\n",
    "    for i_epoch in range(epochs):\n",
    "        print(\"Epoch {} -----\\n\".format(i_epoch+1))\n",
    "        mse_, mape_ = train(train_dataloader, model, loss_fn, optimizer)\n",
    "        mse_record.extend(mse_)\n",
    "        mse_epochs.append(mse_[-1])\n",
    "        mape_record.extend(mape_)\n",
    "        mape_epochs.append(mape_[-1])\n",
    "\n",
    "    min_mse_all = min(mse_record)\n",
    "    avg_mse_all = torch.mean(torch.stack(mse_record))\n",
    "    min_mape_all = min(mse_record)\n",
    "    avg_mape_all = torch.mean(torch.stack(mape_record))\n",
    "\n",
    "    print(\"Min. MSE : {} \".format(min_mse_all)\n",
    "    print(\"Avg. MSE : {} \".format(avg_mse_all)\n",
    "    print(\"Min. MAPE : {} \".format(min_mape_all)\n",
    "    print(\"Avg. MAPE : {} \".format(avg_mape_all)\n",
    "    \n",
    "    print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
