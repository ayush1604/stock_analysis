{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view as sliding_window_view\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "\n",
    "class NSEDataset(Dataset):\n",
    "    def __init__(self, ohlcv_dir, target_ticker, target_ticker_file, len_window, len_corr_traceback, nP, nN, \n",
    "    keep_tickers=None, ohlcv_prefix='', ohlcv_sufix='', ohlcv_files=None, start_date=None, end_date=None,\n",
    "    target_feat='c', keep_feat='ohlcva'):\n",
    "\n",
    "        feat_name_map = {\n",
    "            'o' : 'Open', \n",
    "            'h' : 'High', \n",
    "            'l' : 'Low', \n",
    "            'c' : 'Close', \n",
    "            'v' : 'Volume',\n",
    "            'a' : 'Adj Close'\n",
    "        }\n",
    "\n",
    "        self.len_window = len_window\n",
    "        self.len_corr_traceback = len_corr_traceback\n",
    "        self.nP, self.nN = nP, nN\n",
    "        self.target_feat = target_feat\n",
    "        self.keep_feat = keep_feat\n",
    "        self.start_date, self.end_date = start_date, end_date\n",
    "\n",
    "        if ohlcv_files is not None:\n",
    "            ohlcv_files = set(ohlcv_files)\n",
    "        \n",
    "        if keep_tickers is not None:\n",
    "            keep_tickers = set(keep_tickers)\n",
    "    \n",
    "        df = pd.read_csv(os.path.join(ohlcv_dir, target_ticker_file))\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "        if start_date is not None:\n",
    "            start_mask =  df['Date'] >= datetime.datetime.fromisoformat(start_date)\n",
    "            i_start = start_mask[start_mask].index.min()\n",
    "        else:\n",
    "            i_start = 0\n",
    "        \n",
    "        if end_date is not None:\n",
    "            end_mask =  df['Date'] > datetime.datetime.fromisoformat(end_date)\n",
    "            i_end = end_mask[end_mask].index.min()\n",
    "        else:\n",
    "            i_end = len(df)\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.set_index(['Date', 'Ticker'], inplace=True)\n",
    "        df = df.iloc[i_start:i_end]\n",
    "        self.mainstream_df = df.loc[:, target_ticker]\n",
    "        self.df = df.drop(target_ticker, axis=1)\n",
    "        \n",
    "        if ohlcv_files is not None and target_ticker_file not in ohlcv_files:\n",
    "            self.df = pd.DataFrame(columns=df.columns)\n",
    "            \n",
    "        for f in os.listdir(ohlcv_dir):\n",
    "            if f.startswith(ohlcv_prefix) and f.endswith(ohlcv_sufix) and (ohlcv_files is None or f in ohlcv_files):\n",
    "                if f == target_ticker_file:\n",
    "                    continue\n",
    "                else:\n",
    "                    temp_df = pd.read_csv(os.path.join(ohlcv_dir, f))\n",
    "                    temp_df.reset_index(drop=True, inplace=True)\n",
    "                    temp_df['Date'] = pd.to_datetime(temp_df['Date'])\n",
    "                    self.df = pd.merge(self.df.reset_index(), temp_df, on=['Date', 'Ticker'],\n",
    "                    how='inner', suffixes=('', '_y')).set_index(['Date', 'Ticker'])\n",
    "                    self.df.drop(self.df.filter(regex='_y$').columns.tolist(), axis=1, inplace=True)\n",
    "                    \n",
    "        if keep_tickers is not None:\n",
    "            for c in self.df:\n",
    "                if c not in keep_tickers:\n",
    "                    self.df.drop(c, axis=1, inplace=True)\n",
    "\n",
    "        self.df = self.df.pivot_table(index='Date', columns='Ticker')\n",
    "        \n",
    "        self.df.columns = self.df.columns.map('_'.join)\n",
    "        if isinstance(self.mainstream_df, pd.Series):\n",
    "            self.mainstream_df = pd.DataFrame(self.mainstream_df)\n",
    "        \n",
    "        self.mainstream_df = self.mainstream_df.pivot_table(index='Date', columns='Ticker')\n",
    "        self.mainstream_df.columns = self.mainstream_df.columns.map('_'.join)\n",
    "\n",
    "        target_feat_name = \"{}_{}\".format(target_ticker, feat_name_map[target_feat])\n",
    "        \n",
    "        self.unshifted_target = self.mainstream_df.loc[:, target_feat_name]\n",
    "        self.target = self.unshifted_target.shift(periods=-1)\n",
    "        \n",
    "        # To account for absence of target for last row.\n",
    "        self.df = self.df.iloc[:-1, :]  \n",
    "        self.mainstream_df = self.mainstream_df.iloc[:-1, :]\n",
    "\n",
    "        drop_features = set(feat_name_map.keys()).difference({feat for feat in keep_feat})\n",
    "        for feat in drop_features:\n",
    "            self.df.drop(self.df.filter(regex='_{}$'.format(feat_name_map[feat])).columns.tolist(), axis=1, inplace=True)\n",
    "            self.mainstream_df.drop(self.mainstream_df.filter(regex='_{}$'.format(feat_name_map[feat])).columns.tolist(), axis=1, inplace=True)\n",
    "\n",
    "        # For i_end, data of [i_end - (len_corr_traceback) : i_end] (py notation)\n",
    "        # is needed to calculate correlation, basis on which data of \n",
    "        # [i_end - (len_window) : i_end] (py notation) must be sent.\n",
    "        # i_end is excluded, so last i_end should be len(self.df)\n",
    "        \n",
    "        self.swdf = []\n",
    "        for i_end in range(len_corr_traceback, len(self.df)+1):\n",
    "            if i_end % 50 == 0:\n",
    "                print(i_end, len(self.df))\n",
    "            self.swdf.append(self.get_high_corr(self.unshifted_target.iloc[i_end-len_corr_traceback:i_end], \n",
    "            self.df.iloc[i_end-len_corr_traceback:i_end, :], len_window, nP, nN))\n",
    "            \n",
    "        # self.swdf = np.array(self.swdf).reshape(len(self.swdf), self.len_window, -1)\n",
    "        self.swdf = np.array(self.swdf)\n",
    "        \n",
    "        # if earlier self.df.shape was (6(n+1), c), it should now be\n",
    "        # (n, c), mainstream_df.shape and index_data_df should be (n, 1) and swdf.shape\n",
    "        # should be (n-lct+1, lw*(nP+nN)).\n",
    "\n",
    "        # For index 0, \n",
    "        # swdf[0], mainstream_df[lct-lw : lct] flattend (py notation)\n",
    "        # index_data_df[lct-lw : lct] flattened (py notation) should be accessed.\n",
    "\n",
    "        # For index i < len(swdf), \n",
    "        # swdf[i], mainstream_df[lct-lw + i: lct + i] flattend (py notation)\n",
    "        # index_data_df[lct-lw + i : lct + i] flattened (py notation) should be accessed.\n",
    "        # Correct. Continue from here. \n",
    "\n",
    "        # The index data used is for a single index.\n",
    "        self.index_data_df = pd.read_csv(\"data_collection/NIFTY 50.csv\")\n",
    "        self.index_data_df['Date'] = pd.to_datetime(self.index_data_df['Date'])\n",
    "        self.index_data_df.rename(columns={'SharesTraded' : 'Volume'}, inplace=True)\n",
    "        self.index_data_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "        if start_date is not None:\n",
    "            start_mask =  self.index_data_df['Date'] >= datetime.datetime.fromisoformat(start_date)\n",
    "            i_start = start_mask[start_mask].index.min()\n",
    "        else:\n",
    "            i_start = 0\n",
    "\n",
    "        if end_date is not None:\n",
    "            end_mask =  self.index_data_df['Date'] > datetime.datetime.fromisoformat(end_date)\n",
    "            i_end = end_mask[end_mask].index.min()\n",
    "        else:\n",
    "            i_end = len(self.index_data_df)\n",
    "\n",
    "        self.index_data_df.reset_index(drop=True, inplace=True)\n",
    "        self.index_data_df.set_index(['Date'], inplace=True)\n",
    "        self.index_data_df = self.index_data_df.iloc[i_start:i_end]\n",
    "        self.index_data_df = pd.DataFrame(self.index_data_df.loc[:, 'Close'])\n",
    "        self.index_data_df = self.index_data_df.reset_index().merge(\n",
    "    self.df.reset_index()['Date'], how='inner', on='Date').set_index('Date')\n",
    "        \n",
    "\n",
    "    def get_high_corr(self, target: pd.Series, candidates: pd.DataFrame, len_window, nP, nN):\n",
    "        corr = candidates.corrwith(target)\n",
    "        p_best = corr.nlargest(nP)\n",
    "        n_best = corr.nsmallest(nN)\n",
    "        newrow = candidates.iloc[-len_window:, candidates.columns.get_indexer(p_best.index)].melt()['value'].tolist()\n",
    "        newrow.extend(candidates.iloc[-len_window:, candidates.columns.get_indexer(n_best.index)].melt()['value'].tolist())\n",
    "        return newrow\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.swdf)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For index i < len(swdf), \n",
    "        # swdf[i], mainstream_df[lct-lw + i: lct + i] flattend (py notation)\n",
    "        # index_data_df[lct-lw + i : lct + i] flattened (py notation) should be accessed.\n",
    "        \n",
    "        return (self.swdf[idx, :].reshape(dataset.len_window, -1), \n",
    "        self.mainstream_df.iloc[self.len_corr_traceback-self.len_window+idx : self.len_corr_traceback+idx].to_numpy(), \n",
    "        self.index_data_df.iloc[self.len_corr_traceback-self.len_window+idx : self.len_corr_traceback+idx].to_numpy())\n",
    "\n",
    "\n",
    "def save_NSEDataset(dataset, opfile):\n",
    "    with open(opfile, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "def load_NSEDataset(ipfile):\n",
    "    PICKLE_PROTOCOL = 4\n",
    "    with open(ipfile, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50 1285\n",
      "100 1285\n",
      "150 1285\n",
      "200 1285\n",
      "250 1285\n",
      "300 1285\n",
      "350 1285\n",
      "400 1285\n",
      "450 1285\n",
      "500 1285\n",
      "550 1285\n",
      "600 1285\n",
      "650 1285\n",
      "700 1285\n",
      "750 1285\n",
      "800 1285\n",
      "850 1285\n",
      "900 1285\n",
      "950 1285\n",
      "1000 1285\n",
      "1050 1285\n",
      "1100 1285\n",
      "1150 1285\n",
      "1200 1285\n",
      "1250 1285\n"
     ]
    }
   ],
   "source": [
    "dataset = NSEDataset('data_collection/ohlcv_data', 'ITC.NS', 'ohlcv_fmcg.csv', len_window=10, len_corr_traceback=20, \n",
    "nP=10, nN=10, keep_feat='o', start_date='2017-01-01')\n",
    "save_NSEDataset(dataset, 'data_collection/pickled_datasets/itc_Jan17_w10_t20_p10_n10_o.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_NSEDataset('data_collection/pickled_datasets/itc_Jan17_w10_t20_p10_n10_o.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1266, 200) (1285, 1) (1285, 1)\n(10, 20) (10, 1) (10, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.swdf.shape, dataset.mainstream_df.shape, dataset.index_data_df.shape)\n",
    "print(dataset[1][0].shape, dataset[1][1].shape, dataset[1][2].shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d956a392b67227f401c129a901ef5c98887812674686cb9105f1c9b415cc849"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}